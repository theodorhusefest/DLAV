{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() + '/data/ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1224a9dd0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(y, x, theta):\n",
    "    e = (y - np.dot(x, theta))\n",
    "    \n",
    "    return (1/len(y))*e.T @ e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.14546775491134"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(y, X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    return (-1/len(y))*(tx.T@(y-tx@w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        grad = compute_gradient(y, X, theta)\n",
    "        loss = computeCost(y, X, theta)\n",
    "        # ***************************************************\n",
    "        theta = theta - alpha*grad\n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=64.14546775491134, w0=0.05839135051546391, w1=0.6532884974555669\n",
      "Gradient Descent(1/999): loss=13.474380929740022, w0=0.06289175271039384, w1=0.7700097825599364\n",
      "Gradient Descent(2/999): loss=11.863187137209913, w0=0.05782292746142813, w1=0.7913481156584673\n",
      "Gradient Descent(3/999): loss=11.802309414162776, w0=0.05106362516077815, w1=0.795729810284954\n",
      "Gradient Descent(4/999): loss=11.790457172888441, w0=0.044014378365002604, w1=0.7970961782721866\n",
      "Gradient Descent(5/999): loss=11.78018988623466, w0=0.036924131142162614, w1=0.7979254732843951\n",
      "Gradient Descent(6/999): loss=11.77000831688729, w0=0.029837117577144835, w1=0.7986582394519285\n",
      "Gradient Descent(7/999): loss=11.759864960982835, w0=0.02276118189403884, w1=0.7993727912003019\n",
      "Gradient Descent(8/999): loss=11.749758189525146, w0=0.015697699574200134, w1=0.8000830518518655\n",
      "Gradient Descent(9/999): loss=11.739687823612774, w0=0.008646896228913532, w1=0.8007914983590768\n",
      "Gradient Descent(10/999): loss=11.72965373062586, w0=0.001608793098984364, w1=0.8014985729280016\n",
      "Gradient Descent(11/999): loss=11.71965577986436, w0=-0.005416624870320639, w1=0.8022043560583255\n",
      "Gradient Descent(12/999): loss=11.709693841144578, w0=-0.01242937915180076, w1=0.8029088639482521\n",
      "Gradient Descent(13/999): loss=11.699767784753174, w0=-0.019429492325268312, w1=0.8036121013621462\n",
      "Gradient Descent(14/999): loss=11.689877481444068, w0=-0.026416987133500117, w1=0.8043140710284592\n",
      "Gradient Descent(15/999): loss=11.68002280243672, w0=-0.03339188631448141, w1=0.8050147753103406\n",
      "Gradient Descent(16/999): loss=11.670203619414455, w0=-0.04035421257164585, w1=0.8057142165026173\n",
      "Gradient Descent(17/999): loss=11.660419804522776, w0=-0.04730398856864604, w1=0.8064123968845912\n",
      "Gradient Descent(18/999): loss=11.650671230367724, w0=-0.05424123692848453, w1=0.8071093187294315\n",
      "Gradient Descent(19/999): loss=11.640957770014198, w0=-0.06116598023341992, w1=0.8078049843058496\n",
      "Gradient Descent(20/999): loss=11.631279296984308, w0=-0.06807824102501052, w1=0.8084993958784037\n",
      "Gradient Descent(21/999): loss=11.621635685255738, w0=-0.07497804180418248, w1=0.8091925557075582\n",
      "Gradient Descent(22/999): loss=11.612026809260087, w0=-0.08186540503130207, w1=0.8098844660497012\n",
      "Gradient Descent(23/999): loss=11.602452543881256, w0=-0.08874035312624866, w1=0.8105751291571527\n",
      "Gradient Descent(24/999): loss=11.592912764453798, w0=-0.0956029084684876, w1=0.8112645472781728\n",
      "Gradient Descent(25/999): loss=11.583407346761305, w0=-0.10245309339714315, w1=0.8119527226569687\n",
      "Gradient Descent(26/999): loss=11.573936167034791, w0=-0.10929093021107113, w1=0.8126396575337025\n",
      "Gradient Descent(27/999): loss=11.564499101951078, w0=-0.11611644116893155, w1=0.813325354144498\n",
      "Gradient Descent(28/999): loss=11.555096028631192, w0=-0.12292964848926106, w1=0.8140098147214483\n",
      "Gradient Descent(29/999): loss=11.545726824638763, w0=-0.12973057435054527, w1=0.8146930414926226\n",
      "Gradient Descent(30/999): loss=11.536391367978426, w0=-0.13651924089129092, w1=0.8153750366820742\n",
      "Gradient Descent(31/999): loss=11.52708953709424, w0=-0.14329567021009798, w1=0.8160558025098472\n",
      "Gradient Descent(32/999): loss=11.517821210868096, w0=-0.1500598843657316, w1=0.8167353411919838\n",
      "Gradient Descent(33/999): loss=11.508586268618153, w0=-0.15681190537719386, w1=0.8174136549405314\n",
      "Gradient Descent(34/999): loss=11.499384590097256, w0=-0.16355175522379548, w1=0.8180907459635505\n",
      "Gradient Descent(35/999): loss=11.490216055491366, w0=-0.17027945584522738, w1=0.8187666164651208\n",
      "Gradient Descent(36/999): loss=11.481080545418024, w0=-0.17699502914163212, w1=0.8194412686453493\n",
      "Gradient Descent(37/999): loss=11.471977940924763, w0=-0.1836984969736751, w1=0.8201147047003767\n",
      "Gradient Descent(38/999): loss=11.462908123487583, w0=-0.19038988116261576, w1=0.8207869268223856\n",
      "Gradient Descent(39/999): loss=11.453870975009401, w0=-0.1970692034903787, w1=0.8214579371996062\n",
      "Gradient Descent(40/999): loss=11.444866377818512, w0=-0.20373648569962446, w1=0.822127738016325\n",
      "Gradient Descent(41/999): loss=11.435894214667057, w0=-0.2103917494938204, w1=0.8227963314528903\n",
      "Gradient Descent(42/999): loss=11.426954368729499, w0=-0.21703501653731122, w1=0.8234637196857207\n",
      "Gradient Descent(43/999): loss=11.418046723601098, w0=-0.22366630845538962, w1=0.8241299048873114\n",
      "Gradient Descent(44/999): loss=11.409171163296396, w0=-0.23028564683436664, w1=0.8247948892262416\n",
      "Gradient Descent(45/999): loss=11.400327572247713, w0=-0.23689305322164192, w1=0.8254586748671812\n",
      "Gradient Descent(46/999): loss=11.391515835303629, w0=-0.24348854912577383, w1=0.8261212639708981\n",
      "Gradient Descent(47/999): loss=11.382735837727502, w0=-0.25007215601654953, w1=0.8267826586942654\n",
      "Gradient Descent(48/999): loss=11.37398746519596, w0=-0.25664389532505477, w1=0.8274428611902682\n",
      "Gradient Descent(49/999): loss=11.365270603797414, w0=-0.2632037884437438, w1=0.8281018736080105\n",
      "Gradient Descent(50/999): loss=11.356585140030584, w0=-0.2697518567265089, w1=0.8287596980927223\n",
      "Gradient Descent(51/999): loss=11.347930960803012, w0=-0.27628812148874987, w1=0.8294163367857669\n",
      "Gradient Descent(52/999): loss=11.339307953429595, w0=-0.28281260400744346, w1=0.8300717918246473\n",
      "Gradient Descent(53/999): loss=11.330716005631109, w0=-0.28932532552121265, w1=0.8307260653430135\n",
      "Gradient Descent(54/999): loss=11.322155005532759, w0=-0.2958263072303958, w1=0.8313791594706695\n",
      "Gradient Descent(55/999): loss=11.313624841662717, w0=-0.30231557029711564, w1=0.8320310763335801\n",
      "Gradient Descent(56/999): loss=11.305125402950667, w0=-0.308793135845348, w1=0.8326818180538778\n",
      "Gradient Descent(57/999): loss=11.296656578726363, w0=-0.31525902496099095, w1=0.8333313867498697\n",
      "Gradient Descent(58/999): loss=11.288218258718185, w0=-0.321713258691933, w1=0.8339797845360446\n",
      "Gradient Descent(59/999): loss=11.27981033305171, w0=-0.3281558580481219, w1=0.8346270135230796\n",
      "Gradient Descent(60/999): loss=11.271432692248274, w0=-0.334586844001633, w1=0.835273075817847\n",
      "Gradient Descent(61/999): loss=11.263085227223543, w0=-0.3410062374867375, w1=0.8359179735234217\n",
      "Gradient Descent(62/999): loss=11.254767829286106, w0=-0.34741405939997033, w1=0.8365617087390872\n",
      "Gradient Descent(63/999): loss=11.246480390136053, w0=-0.35381033060019873, w1=0.8372042835603428\n",
      "Gradient Descent(64/999): loss=11.238222801863557, w0=-0.36019507190868966, w1=0.8378457000789108\n",
      "Gradient Descent(65/999): loss=11.22999495694748, w0=-0.3665683041091778, w1=0.8384859603827427\n",
      "Gradient Descent(66/999): loss=11.221796748253968, w0=-0.3729300479479331, w1=0.8391250665560263\n",
      "Gradient Descent(67/999): loss=11.213628069035062, w0=-0.3792803241338285, w1=0.8397630206791926\n",
      "Gradient Descent(68/999): loss=11.205488812927294, w0=-0.3856191533384071, w1=0.8403998248289224\n",
      "Gradient Descent(69/999): loss=11.197378873950317, w0=-0.3919465561959495, w1=0.8410354810781527\n",
      "Gradient Descent(70/999): loss=11.18929814650552, w0=-0.39826255330354116, w1=0.8416699914960846\n",
      "Gradient Descent(71/999): loss=11.181246525374645, w0=-0.40456716522113934, w1=0.8423033581481884\n",
      "Gradient Descent(72/999): loss=11.173223905718434, w0=-0.4108604124716399, w1=0.8429355830962117\n",
      "Gradient Descent(73/999): loss=11.165230183075245, w0=-0.4171423155409443, w1=0.8435666683981857\n",
      "Gradient Descent(74/999): loss=11.157265253359707, w0=-0.4234128948780261, w1=0.8441966161084314\n",
      "Gradient Descent(75/999): loss=11.149329012861354, w0=-0.4296721708949977, w1=0.844825428277567\n",
      "Gradient Descent(76/999): loss=11.141421358243285, w0=-0.4359201639671767, w1=0.8454531069525142\n",
      "Gradient Descent(77/999): loss=11.133542186540808, w0=-0.44215689443315226, w1=0.8460796541765048\n",
      "Gradient Descent(78/999): loss=11.1256913951601, w0=-0.44838238259485125, w1=0.8467050719890875\n",
      "Gradient Descent(79/999): loss=11.117868881876884, w0=-0.45459664871760436, w1=0.8473293624261348\n",
      "Gradient Descent(80/999): loss=11.110074544835086, w0=-0.46079971303021217, w1=0.8479525275198488\n",
      "Gradient Descent(81/999): loss=11.102308282545508, w0=-0.46699159572501076, w1=0.8485745692987691\n",
      "Gradient Descent(82/999): loss=11.094569993884512, w0=-0.4731723169579377, w1=0.8491954897877779\n",
      "Gradient Descent(83/999): loss=11.086859578092703, w0=-0.4793418968485975, w1=0.8498152910081078\n",
      "Gradient Descent(84/999): loss=11.079176934773617, w0=-0.4855003554803272, w1=0.8504339749773481\n",
      "Gradient Descent(85/999): loss=11.071521963892407, w0=-0.49164771290026166, w1=0.8510515437094506\n",
      "Gradient Descent(86/999): loss=11.06389456577455, w0=-0.49778398911939886, w1=0.8516679992147372\n",
      "Gradient Descent(87/999): loss=11.056294641104543, w0=-0.503909204112665, w1=0.8522833434999061\n",
      "Gradient Descent(88/999): loss=11.048722090924613, w0=-0.5100233778189798, w1=0.8528975785680377\n",
      "Gradient Descent(89/999): loss=11.041176816633426, w0=-0.5161265301413208, w1=0.8535107064186024\n",
      "Gradient Descent(90/999): loss=11.033658719984805, w0=-0.5222186809467888, w1=0.8541227290474656\n",
      "Gradient Descent(91/999): loss=11.02616770308645, w0=-0.5282998500666721, w1=0.8547336484468955\n",
      "Gradient Descent(92/999): loss=11.018703668398663, w0=-0.5343700572965112, w1=0.8553434666055688\n",
      "Gradient Descent(93/999): loss=11.011266518733077, w0=-0.5404293223961634, w1=0.8559521855085775\n",
      "Gradient Descent(94/999): loss=11.003856157251398, w0=-0.5464776650898667, w1=0.8565598071374354\n",
      "Gradient Descent(95/999): loss=10.99647248746413, w0=-0.5525151050663046, w1=0.8571663334700841\n",
      "Gradient Descent(96/999): loss=10.989115413229332, w0=-0.5585416619786696, w1=0.8577717664809001\n",
      "Gradient Descent(97/999): loss=10.981784838751356, w0=-0.5645573554447275, w1=0.8583761081407008\n",
      "Gradient Descent(98/999): loss=10.97448066857961, w0=-0.5705622050468813, w1=0.8589793604167508\n",
      "Gradient Descent(99/999): loss=10.967202807607306, w0=-0.5765562303322346, w1=0.8595815252727688\n",
      "Gradient Descent(100/999): loss=10.959951161070222, w0=-0.5825394508126557, w1=0.8601826046689336\n",
      "Gradient Descent(101/999): loss=10.952725634545484, w0=-0.5885118859648408, w1=0.8607826005618906\n",
      "Gradient Descent(102/999): loss=10.945526133950306, w0=-0.5944735552303777, w1=0.8613815149047582\n",
      "Gradient Descent(103/999): loss=10.938352565540795, w0=-0.6004244780158084, w1=0.861979349647134\n",
      "Gradient Descent(104/999): loss=10.931204835910716, w0=-0.6063646736926933, w1=0.8625761067351013\n",
      "Gradient Descent(105/999): loss=10.924082851990278, w0=-0.6122941615976732, w1=0.8631717881112356\n",
      "Gradient Descent(106/999): loss=10.916986521044915, w0=-0.6182129610325332, w1=0.8637663957146104\n",
      "Gradient Descent(107/999): loss=10.909915750674095, w0=-0.6241210912642647, w1=0.864359931480804\n",
      "Gradient Descent(108/999): loss=10.902870448810104, w0=-0.6300185715251289, w1=0.8649523973419058\n",
      "Gradient Descent(109/999): loss=10.895850523716849, w0=-0.6359054210127185, w1=0.865543795226522\n",
      "Gradient Descent(110/999): loss=10.888855883988668, w0=-0.6417816588900211, w1=0.8661341270597827\n",
      "Gradient Descent(111/999): loss=10.881886438549127, w0=-0.6476473042854811, w1=0.8667233947633476\n",
      "Gradient Descent(112/999): loss=10.874942096649868, w0=-0.6535023762930621, w1=0.8673116002554123\n",
      "Gradient Descent(113/999): loss=10.868022767869373, w0=-0.6593468939723086, w1=0.8678987454507151\n",
      "Gradient Descent(114/999): loss=10.861128362111838, w0=-0.6651808763484091, w1=0.8684848322605423\n",
      "Gradient Descent(115/999): loss=10.854258789605971, w0=-0.6710043424122568, w1=0.8690698625927351\n",
      "Gradient Descent(116/999): loss=10.847413960903836, w0=-0.6768173111205124, w1=0.8696538383516959\n",
      "Gradient Descent(117/999): loss=10.840593786879676, w0=-0.682619801395665, w1=0.8702367614383937\n",
      "Gradient Descent(118/999): loss=10.833798178728765, w0=-0.6884118321260945, w1=0.8708186337503714\n",
      "Gradient Descent(119/999): loss=10.827027047966247, w0=-0.6941934221661324, w1=0.8713994571817509\n",
      "Gradient Descent(120/999): loss=10.820280306425975, w0=-0.6999645903361237, w1=0.8719792336232401\n",
      "Gradient Descent(121/999): loss=10.813557866259387, w0=-0.7057253554224877, w1=0.8725579649621387\n",
      "Gradient Descent(122/999): loss=10.806859639934332, w0=-0.7114757361777795, w1=0.873135653082344\n",
      "Gradient Descent(123/999): loss=10.80018554023395, w0=-0.7172157513207509, w1=0.8737122998643578\n",
      "Gradient Descent(124/999): loss=10.793535480255535, w0=-0.7229454195364113, w1=0.874287907185292\n",
      "Gradient Descent(125/999): loss=10.786909373409394, w0=-0.7286647594760888, w1=0.8748624769188748\n",
      "Gradient Descent(126/999): loss=10.780307133417722, w0=-0.7343737897574903, w1=0.8754360109354568\n",
      "Gradient Descent(127/999): loss=10.773728674313494, w0=-0.7400725289647629, w1=0.876008511102017\n",
      "Gradient Descent(128/999): loss=10.767173910439322, w0=-0.7457609956485538, w1=0.8765799792821695\n",
      "Gradient Descent(129/999): loss=10.760642756446355, w0=-0.7514392083260708, w1=0.8771504173361683\n",
      "Gradient Descent(130/999): loss=10.754135127293164, w0=-0.7571071854811429, w1=0.8777198271209147\n",
      "Gradient Descent(131/999): loss=10.747650938244634, w0=-0.7627649455642799, w1=0.8782882104899624\n",
      "Gradient Descent(132/999): loss=10.741190104870858, w0=-0.7684125069927331, w1=0.8788555692935243\n",
      "Gradient Descent(133/999): loss=10.734752543046048, w0=-0.7740498881505549, w1=0.8794219053784776\n",
      "Gradient Descent(134/999): loss=10.728338168947424, w0=-0.7796771073886584, w1=0.8799872205883708\n",
      "Gradient Descent(135/999): loss=10.721946899054137, w0=-0.7852941830248777, w1=0.8805515167634288\n",
      "Gradient Descent(136/999): loss=10.71557865014617, w0=-0.7909011333440273, w1=0.8811147957405598\n",
      "Gradient Descent(137/999): loss=10.709233339303264, w0=-0.7964979765979614, w1=0.8816770593533604\n",
      "Gradient Descent(138/999): loss=10.702910883903831, w0=-0.8020847310056334, w1=0.882238309432122\n",
      "Gradient Descent(139/999): loss=10.696611201623885, w0=-0.8076614147531554, w1=0.8827985478038369\n",
      "Gradient Descent(140/999): loss=10.690334210435964, w0=-0.8132280459938575, w1=0.8833577762922041\n",
      "Gradient Descent(141/999): loss=10.68407982860806, w0=-0.8187846428483462, w1=0.883915996717635\n",
      "Gradient Descent(142/999): loss=10.677847974702566, w0=-0.8243312234045643, w1=0.8844732108972596\n",
      "Gradient Descent(143/999): loss=10.671638567575204, w0=-0.8298678057178493, w1=0.8850294206449325\n",
      "Gradient Descent(144/999): loss=10.665451526373976, w0=-0.8353944078109922, w1=0.8855846277712385\n",
      "Gradient Descent(145/999): loss=10.659286770538108, w0=-0.8409110476742958, w1=0.8861388340834986\n",
      "Gradient Descent(146/999): loss=10.653144219796996, w0=-0.8464177432656342, w1=0.8866920413857761\n",
      "Gradient Descent(147/999): loss=10.647023794169176, w0=-0.8519145125105105, w1=0.8872442514788821\n",
      "Gradient Descent(148/999): loss=10.64092541396126, w0=-0.8574013733021153, w1=0.8877954661603819\n",
      "Gradient Descent(149/999): loss=10.63484899976692, w0=-0.862878343501385, w1=0.8883456872246002\n",
      "Gradient Descent(150/999): loss=10.628794472465847, w0=-0.8683454409370602, w1=0.8888949164626272\n",
      "Gradient Descent(151/999): loss=10.622761753222711, w0=-0.8738026834057432, w1=0.8894431556623249\n",
      "Gradient Descent(152/999): loss=10.616750763486145, w0=-0.8792500886719562, w1=0.8899904066083322\n",
      "Gradient Descent(153/999): loss=10.610761424987722, w0=-0.8846876744681994, w1=0.8905366710820709\n",
      "Gradient Descent(154/999): loss=10.60479365974093, w0=-0.8901154584950083, w1=0.891081950861752\n",
      "Gradient Descent(155/999): loss=10.598847390040163, w0=-0.8955334584210115, w1=0.8916262477223807\n",
      "Gradient Descent(156/999): loss=10.592922538459707, w0=-0.9009416918829883, w1=0.892169563435763\n",
      "Gradient Descent(157/999): loss=10.58701902785272, w0=-0.9063401764859259, w1=0.8927118997705107\n",
      "Gradient Descent(158/999): loss=10.581136781350256, w0=-0.9117289298030768, w1=0.8932532584920476\n",
      "Gradient Descent(159/999): loss=10.575275722360239, w0=-0.9171079693760162, w1=0.8937936413626153\n",
      "Gradient Descent(160/999): loss=10.569435774566463, w0=-0.9224773127146988, w1=0.8943330501412785\n",
      "Gradient Descent(161/999): loss=10.563616861927622, w0=-0.9278369772975159, w1=0.8948714865839313\n",
      "Gradient Descent(162/999): loss=10.557818908676307, w0=-0.9331869805713524, w1=0.8954089524433027\n",
      "Gradient Descent(163/999): loss=10.552041839318003, w0=-0.9385273399516436, w1=0.8959454494689617\n",
      "Gradient Descent(164/999): loss=10.54628557863014, w0=-0.9438580728224316, w1=0.8964809794073243\n",
      "Gradient Descent(165/999): loss=10.540550051661086, w0=-0.9491791965364222, w1=0.8970155440016577\n",
      "Gradient Descent(166/999): loss=10.534835183729186, w0=-0.9544907284150413, w1=0.8975491449920872\n",
      "Gradient Descent(167/999): loss=10.52914090042177, w0=-0.9597926857484913, w1=0.898081784115601\n",
      "Gradient Descent(168/999): loss=10.52346712759422, w0=-0.9650850857958073, w1=0.8986134631060566\n",
      "Gradient Descent(169/999): loss=10.517813791368965, w0=-0.9703679457849134, w1=0.8991441836941857\n",
      "Gradient Descent(170/999): loss=10.512180818134548, w0=-0.9756412829126785, w1=0.8996739476076003\n",
      "Gradient Descent(171/999): loss=10.506568134544644, w0=-0.9809051143449727, w1=0.9002027565707984\n",
      "Gradient Descent(172/999): loss=10.500975667517128, w0=-0.9861594572167232, w1=0.900730612305169\n",
      "Gradient Descent(173/999): loss=10.495403344233111, w0=-0.9914043286319691, w1=0.9012575165289988\n",
      "Gradient Descent(174/999): loss=10.48985109213599, w0=-0.9966397456639188, w1=0.9017834709574764\n",
      "Gradient Descent(175/999): loss=10.484318838930504, w0=-1.0018657253550038, w1=0.902308477302699\n",
      "Gradient Descent(176/999): loss=10.478806512581805, w0=-1.0070822847169356, w1=0.9028325372736775\n",
      "Gradient Descent(177/999): loss=10.473314041314502, w0=-1.0122894407307599, w1=0.9033556525763423\n",
      "Gradient Descent(178/999): loss=10.467841353611739, w0=-1.0174872103469128, w1=0.9038778249135484\n",
      "Gradient Descent(179/999): loss=10.462388378214257, w0=-1.0226756104852754, w1=0.9043990559850815\n",
      "Gradient Descent(180/999): loss=10.456955044119471, w0=-1.0278546580352295, w1=0.9049193474876631\n",
      "Gradient Descent(181/999): loss=10.451541280580543, w0=-1.0330243698557116, w1=0.9054387011149563\n",
      "Gradient Descent(182/999): loss=10.446147017105456, w0=-1.0381847627752687, w1=0.9059571185575711\n",
      "Gradient Descent(183/999): loss=10.440772183456112, w0=-1.043335853592113, w1=0.9064746015030702\n",
      "Gradient Descent(184/999): loss=10.435416709647392, w0=-1.0484776590741753, w1=0.9069911516359741\n",
      "Gradient Descent(185/999): loss=10.430080525946273, w0=-1.053610195959162, w1=0.907506770637767\n",
      "Gradient Descent(186/999): loss=10.4247635628709, w0=-1.058733480954607, w1=0.9080214601869018\n",
      "Gradient Descent(187/999): loss=10.419465751189687, w0=-1.0638475307379278, w1=0.9085352219588061\n",
      "Gradient Descent(188/999): loss=10.414187021920418, w0=-1.0689523619564794, w1=0.9090480576258873\n",
      "Gradient Descent(189/999): loss=10.408927306329346, w0=-1.0740479912276077, w1=0.9095599688575379\n",
      "Gradient Descent(190/999): loss=10.403686535930298, w0=-1.079134435138705, w1=0.9100709573201414\n",
      "Gradient Descent(191/999): loss=10.39846464248379, w0=-1.084211710247263, w1=0.9105810246770776\n",
      "Gradient Descent(192/999): loss=10.393261557996123, w0=-1.0892798330809266, w1=0.9110901725887276\n",
      "Gradient Descent(193/999): loss=10.38807721471852, w0=-1.0943388201375484, w1=0.9115984027124796\n",
      "Gradient Descent(194/999): loss=10.382911545146214, w0=-1.099388687885242, w1=0.9121057167027342\n",
      "Gradient Descent(195/999): loss=10.377764482017604, w0=-1.1044294527624352, w1=0.91261211621091\n",
      "Gradient Descent(196/999): loss=10.372635958313353, w0=-1.1094611311779248, w1=0.9131176028854483\n",
      "Gradient Descent(197/999): loss=10.367525907255517, w0=-1.1144837395109284, w1=0.9136221783718195\n",
      "Gradient Descent(198/999): loss=10.3624342623067, w0=-1.119497294111139, w1=0.9141258443125273\n",
      "Gradient Descent(199/999): loss=10.357360957169153, w0=-1.1245018112987772, w1=0.9146286023471152\n",
      "Gradient Descent(200/999): loss=10.352305925783932, w0=-1.1294973073646455, w1=0.9151304541121706\n",
      "Gradient Descent(201/999): loss=10.347269102330046, w0=-1.1344837985701801, w1=0.9156314012413316\n",
      "Gradient Descent(202/999): loss=10.342250421223564, w0=-1.1394613011475045, w1=0.9161314453652908\n",
      "Gradient Descent(203/999): loss=10.337249817116808, w0=-1.1444298312994825, w1=0.9166305881118018\n",
      "Gradient Descent(204/999): loss=10.33226722489746, w0=-1.1493894051997706, w1=0.9171288311056836\n",
      "Gradient Descent(205/999): loss=10.32730257968775, w0=-1.1543400389928706, w1=0.9176261759688266\n",
      "Gradient Descent(206/999): loss=10.32235581684358, w0=-1.1592817487941822, w1=0.9181226243201975\n",
      "Gradient Descent(207/999): loss=10.3174268719537, w0=-1.1642145506900559, w1=0.9186181777758449\n",
      "Gradient Descent(208/999): loss=10.312515680838867, w0=-1.1691384607378448, w1=0.9191128379489037\n",
      "Gradient Descent(209/999): loss=10.30762217955101, w0=-1.1740534949659571, w1=0.9196066064496018\n",
      "Gradient Descent(210/999): loss=10.302746304372395, w0=-1.1789596693739082, w1=0.9200994848852637\n",
      "Gradient Descent(211/999): loss=10.29788799181479, w0=-1.183856999932373, w1=0.9205914748603171\n",
      "Gradient Descent(212/999): loss=10.293047178618647, w0=-1.1887455025832374, w1=0.9210825779762974\n",
      "Gradient Descent(213/999): loss=10.28822380175228, w0=-1.193625193239651, w1=0.921572795831853\n",
      "Gradient Descent(214/999): loss=10.28341779841103, w0=-1.1984960877860782, w1=0.9220621300227506\n",
      "Gradient Descent(215/999): loss=10.278629106016467, w0=-1.20335820207835, w1=0.9225505821418805\n",
      "Gradient Descent(216/999): loss=10.273857662215557, w0=-1.2082115519437158, w1=0.9230381537792615\n",
      "Gradient Descent(217/999): loss=10.269103404879864, w0=-1.2130561531808948, w1=0.9235248465220464\n",
      "Gradient Descent(218/999): loss=10.264366272104729, w0=-1.2178920215601279, w1=0.9240106619545269\n",
      "Gradient Descent(219/999): loss=10.259646202208472, w0=-1.222719172823228, w1=0.9244956016581389\n",
      "Gradient Descent(220/999): loss=10.254943133731595, w0=-1.2275376226836328, w1=0.9249796672114675\n",
      "Gradient Descent(221/999): loss=10.25025700543596, w0=-1.2323473868264538, w1=0.9254628601902526\n",
      "Gradient Descent(222/999): loss=10.245587756304015, w0=-1.2371484809085296, w1=0.9259451821673931\n",
      "Gradient Descent(223/999): loss=10.240935325537984, w0=-1.2419409205584753, w1=0.9264266347129532\n",
      "Gradient Descent(224/999): loss=10.236299652559085, w0=-1.246724721376734, w1=0.9269072193941664\n",
      "Gradient Descent(225/999): loss=10.231680677006736, w0=-1.251499898935628, w1=0.9273869377754415\n",
      "Gradient Descent(226/999): loss=10.227078338737769, w0=-1.2562664687794083, w1=0.9278657914183671\n",
      "Gradient Descent(227/999): loss=10.222492577825651, w0=-1.2610244464243061, w1=0.928343781881717\n",
      "Gradient Descent(228/999): loss=10.217923334559696, w0=-1.2657738473585836, w1=0.928820910721455\n",
      "Gradient Descent(229/999): loss=10.213370549444301, w0=-1.2705146870425832, w1=0.9292971794907403\n",
      "Gradient Descent(230/999): loss=10.208834163198151, w0=-1.275246980908779, w1=0.9297725897399323\n",
      "Gradient Descent(231/999): loss=10.204314116753473, w0=-1.2799707443618262, w1=0.9302471430165956\n",
      "Gradient Descent(232/999): loss=10.199810351255238, w0=-1.2846859927786123, w1=0.9307208408655054\n",
      "Gradient Descent(233/999): loss=10.195322808060421, w0=-1.2893927415083057, w1=0.9311936848286523\n",
      "Gradient Descent(234/999): loss=10.190851428737218, w0=-1.294091005872407, w1=0.931665676445247\n",
      "Gradient Descent(235/999): loss=10.1863961550643, w0=-1.2987808011647983, w1=0.932136817251726\n",
      "Gradient Descent(236/999): loss=10.181956929030044, w0=-1.3034621426517927, w1=0.932607108781756\n",
      "Gradient Descent(237/999): loss=10.177533692831775, w0=-1.3081350455721845, w1=0.9330765525662391\n",
      "Gradient Descent(238/999): loss=10.173126388875033, w0=-1.3127995251372988, w1=0.9335451501333181\n",
      "Gradient Descent(239/999): loss=10.168734959772802, w0=-1.3174555965310404, w1=0.9340129030083807\n",
      "Gradient Descent(240/999): loss=10.164359348344771, w0=-1.322103274909944, w1=0.9344798127140652\n",
      "Gradient Descent(241/999): loss=10.159999497616594, w0=-1.326742575403223, w1=0.9349458807702654\n",
      "Gradient Descent(242/999): loss=10.155655350819135, w0=-1.3313735131128188, w1=0.9354111086941351\n",
      "Gradient Descent(243/999): loss=10.151326851387743, w0=-1.3359961031134509, w1=0.9358754980000933\n",
      "Gradient Descent(244/999): loss=10.147013942961511, w0=-1.340610360452664, w1=0.9363390501998291\n",
      "Gradient Descent(245/999): loss=10.142716569382536, w0=-1.345216300150879, w1=0.9368017668023066\n",
      "Gradient Descent(246/999): loss=10.13843467469519, w0=-1.3498139372014408, w1=0.9372636493137702\n",
      "Gradient Descent(247/999): loss=10.13416820314541, w0=-1.3544032865706674, w1=0.9377246992377486\n",
      "Gradient Descent(248/999): loss=10.129917099179936, w0=-1.3589843631978986, w1=0.9381849180750605\n",
      "Gradient Descent(249/999): loss=10.12568130744562, w0=-1.3635571819955445, w1=0.9386443073238195\n",
      "Gradient Descent(250/999): loss=10.121460772788685, w0=-1.3681217578491343, w1=0.9391028684794384\n",
      "Gradient Descent(251/999): loss=10.11725544025402, w0=-1.3726781056173643, w1=0.9395606030346343\n",
      "Gradient Descent(252/999): loss=10.11306525508446, w0=-1.3772262401321467, w1=0.9400175124794337\n",
      "Gradient Descent(253/999): loss=10.108890162720073, w0=-1.3817661761986582, w1=0.9404735983011773\n",
      "Gradient Descent(254/999): loss=10.104730108797435, w0=-1.3862979285953871, w1=0.9409288619845243\n",
      "Gradient Descent(255/999): loss=10.100585039148958, w0=-1.3908215120741825, w1=0.9413833050114583\n",
      "Gradient Descent(256/999): loss=10.096454899802147, w0=-1.3953369413603016, w1=0.9418369288612908\n",
      "Gradient Descent(257/999): loss=10.092339636978913, w0=-1.3998442311524584, w1=0.9422897350106672\n",
      "Gradient Descent(258/999): loss=10.088239197094884, w0=-1.4043433961228702, w1=0.9427417249335708\n",
      "Gradient Descent(259/999): loss=10.084153526758683, w0=-1.408834450917307, w1=0.9431929001013282\n",
      "Gradient Descent(260/999): loss=10.080082572771254, w0=-1.4133174101551382, w1=0.9436432619826134\n",
      "Gradient Descent(261/999): loss=10.076026282125154, w0=-1.4177922884293803, w1=0.9440928120434534\n",
      "Gradient Descent(262/999): loss=10.071984602003877, w0=-1.4222591003067444, w1=0.9445415517472323\n",
      "Gradient Descent(263/999): loss=10.067957479781157, w0=-1.4267178603276838, w1=0.9449894825546963\n",
      "Gradient Descent(264/999): loss=10.06394486302028, w0=-1.4311685830064411, w1=0.9454366059239585\n",
      "Gradient Descent(265/999): loss=10.059946699473413, w0=-1.4356112828310958, w1=0.9458829233105036\n",
      "Gradient Descent(266/999): loss=10.05596293708091, w0=-1.4400459742636114, w1=0.9463284361671929\n",
      "Gradient Descent(267/999): loss=10.051993523970644, w0=-1.444472671739882, w1=0.9467731459442682\n",
      "Gradient Descent(268/999): loss=10.048038408457334, w0=-1.4488913896697795, w1=0.9472170540893577\n",
      "Gradient Descent(269/999): loss=10.044097539041854, w0=-1.4533021424372012, w1=0.9476601620474799\n",
      "Gradient Descent(270/999): loss=10.040170864410586, w0=-1.4577049444001156, w1=0.9481024712610484\n",
      "Gradient Descent(271/999): loss=10.036258333434734, w0=-1.4620998098906095, w1=0.9485439831698768\n",
      "Gradient Descent(272/999): loss=10.03235989516967, w0=-1.466486753214935, w1=0.9489846992111832\n",
      "Gradient Descent(273/999): loss=10.02847549885426, w0=-1.4708657886535559, w1=0.9494246208195952\n",
      "Gradient Descent(274/999): loss=10.02460509391021, w0=-1.4752369304611936, w1=0.9498637494271542\n",
      "Gradient Descent(275/999): loss=10.02074862994142, w0=-1.4796001928668747, w1=0.9503020864633202\n",
      "Gradient Descent(276/999): loss=10.016906056733285, w0=-1.483955590073976, w1=0.9507396333549765\n",
      "Gradient Descent(277/999): loss=10.013077324252096, w0=-1.4883031362602717, w1=0.9511763915264342\n",
      "Gradient Descent(278/999): loss=10.009262382644351, w0=-1.492642845577979, w1=0.9516123623994371\n",
      "Gradient Descent(279/999): loss=10.005461182236122, w0=-1.4969747321538045, w1=0.9520475473931662\n",
      "Gradient Descent(280/999): loss=10.001673673532398, w0=-1.5012988100889901, w1=0.9524819479242441\n",
      "Gradient Descent(281/999): loss=9.997899807216452, w0=-1.5056150934593588, w1=0.9529155654067399\n",
      "Gradient Descent(282/999): loss=9.994139534149191, w0=-1.5099235963153605, w1=0.9533484012521737\n",
      "Gradient Descent(283/999): loss=9.990392805368515, w0=-1.5142243326821179, w1=0.9537804568695212\n",
      "Gradient Descent(284/999): loss=9.986659572088687, w0=-1.5185173165594719, w1=0.9542117336652183\n",
      "Gradient Descent(285/999): loss=9.98293978569969, w0=-1.5228025619220278, w1=0.9546422330431656\n",
      "Gradient Descent(286/999): loss=9.979233397766603, w0=-1.5270800827192, w1=0.9550719564047332\n",
      "Gradient Descent(287/999): loss=9.975540360028958, w0=-1.5313498928752574, w1=0.955500905148765\n",
      "Gradient Descent(288/999): loss=9.971860624400122, w0=-1.53561200628937, w1=0.9559290806715831\n",
      "Gradient Descent(289/999): loss=9.968194142966665, w0=-1.539866436835652, w1=0.956356484366993\n",
      "Gradient Descent(290/999): loss=9.964540867987743, w0=-1.5441131983632095, w1=0.9567831176262876\n",
      "Gradient Descent(291/999): loss=9.960900751894467, w0=-1.5483523046961833, w1=0.9572089818382518\n",
      "Gradient Descent(292/999): loss=9.957273747289296, w0=-1.5525837696337952, w1=0.957634078389167\n",
      "Gradient Descent(293/999): loss=9.953659806945398, w0=-1.5568076069503927, w1=0.9580584086628159\n",
      "Gradient Descent(294/999): loss=9.950058883806062, w0=-1.5610238303954933, w1=0.9584819740404867\n",
      "Gradient Descent(295/999): loss=9.946470930984068, w0=-1.56523245369383, w1=0.9589047759009777\n",
      "Gradient Descent(296/999): loss=9.942895901761082, w0=-1.5694334905453957, w1=0.9593268156206016\n",
      "Gradient Descent(297/999): loss=9.939333749587044, w0=-1.5736269546254877, w1=0.9597480945731905\n",
      "Gradient Descent(298/999): loss=9.935784428079568, w0=-1.577812859584752, w1=0.9601686141300999\n",
      "Gradient Descent(299/999): loss=9.932247891023335, w0=-1.5819912190492285, w1=0.9605883756602133\n",
      "Gradient Descent(300/999): loss=9.928724092369489, w0=-1.5861620466203943, w1=0.9610073805299465\n",
      "Gradient Descent(301/999): loss=9.925212986235039, w0=-1.590325355875209, w1=0.9614256301032524\n",
      "Gradient Descent(302/999): loss=9.92171452690226, w0=-1.5944811603661582, w1=0.9618431257416252\n",
      "Gradient Descent(303/999): loss=9.91822866881811, w0=-1.5986294736212978, w1=0.9622598688041049\n",
      "Gradient Descent(304/999): loss=9.914755366593608, w0=-1.6027703091442982, w1=0.9626758606472816\n",
      "Gradient Descent(305/999): loss=9.911294575003279, w0=-1.6069036804144883, w1=0.9630911026253001\n",
      "Gradient Descent(306/999): loss=9.90784624898454, w0=-1.6110296008868987, w1=0.9635055960898642\n",
      "Gradient Descent(307/999): loss=9.904410343637121, w0=-1.6151480839923065, w1=0.963919342390241\n",
      "Gradient Descent(308/999): loss=9.900986814222481, w0=-1.6192591431372785, w1=0.9643323428732659\n",
      "Gradient Descent(309/999): loss=9.897575616163223, w0=-1.6233627917042146, w1=0.9647445988833457\n",
      "Gradient Descent(310/999): loss=9.894176705042515, w0=-1.6274590430513918, w1=0.9651561117624645\n",
      "Gradient Descent(311/999): loss=9.89079003660352, w0=-1.6315479105130075, w1=0.965566882850187\n",
      "Gradient Descent(312/999): loss=9.887415566748796, w0=-1.635629407399223, w1=0.9659769134836632\n",
      "Gradient Descent(313/999): loss=9.884053251539756, w0=-1.639703546996207, w1=0.966386204997633\n",
      "Gradient Descent(314/999): loss=9.880703047196059, w0=-1.6437703425661778, w1=0.9667947587244299\n",
      "Gradient Descent(315/999): loss=9.877364910095071, w0=-1.6478298073474482, w1=0.9672025759939861\n",
      "Gradient Descent(316/999): loss=9.874038796771284, w0=-1.6518819545544672, w1=0.9676096581338364\n",
      "Gradient Descent(317/999): loss=9.870724663915738, w0=-1.6559267973778633, w1=0.9680160064691224\n",
      "Gradient Descent(318/999): loss=9.867422468375485, w0=-1.6599643489844882, w1=0.9684216223225973\n",
      "Gradient Descent(319/999): loss=9.864132167152999, w0=-1.6639946225174587, w1=0.9688265070146297\n",
      "Gradient Descent(320/999): loss=9.860853717405638, w0=-1.6680176310961998, w1=0.969230661863208\n",
      "Gradient Descent(321/999): loss=9.85758707644507, w0=-1.672033387816488, w1=0.9696340881839453\n",
      "Gradient Descent(322/999): loss=9.854332201736725, w0=-1.6760419057504927, w1=0.9700367872900825\n",
      "Gradient Descent(323/999): loss=9.851089050899247, w0=-1.68004319794682, w1=0.9704387604924937\n",
      "Gradient Descent(324/999): loss=9.847857581703924, w0=-1.6840372774305543, w1=0.9708400090996899\n",
      "Gradient Descent(325/999): loss=9.84463775207416, w0=-1.6880241572033015, w1=0.9712405344178235\n",
      "Gradient Descent(326/999): loss=9.841429520084906, w0=-1.6920038502432302, w1=0.9716403377506925\n",
      "Gradient Descent(327/999): loss=9.838232843962135, w0=-1.6959763695051149, w1=0.9720394203997442\n",
      "Gradient Descent(328/999): loss=9.83504768208228, w0=-1.6999417279203781, w1=0.9724377836640803\n",
      "Gradient Descent(329/999): loss=9.831873992971705, w0=-1.703899938397132, w1=0.9728354288404608\n",
      "Gradient Descent(330/999): loss=9.828711735306154, w0=-1.7078510138202208, w1=0.973232357223308\n",
      "Gradient Descent(331/999): loss=9.825560867910223, w0=-1.7117949670512622, w1=0.9736285701047109\n",
      "Gradient Descent(332/999): loss=9.82242134975682, w0=-1.7157318109286899, w1=0.9740240687744295\n",
      "Gradient Descent(333/999): loss=9.819293139966623, w0=-1.719661558267795, w1=0.9744188545198987\n",
      "Gradient Descent(334/999): loss=9.816176197807568, w0=-1.7235842218607678, w1=0.9748129286262328\n",
      "Gradient Descent(335/999): loss=9.813070482694293, w0=-1.7274998144767395, w1=0.9752062923762296\n",
      "Gradient Descent(336/999): loss=9.809975954187628, w0=-1.7314083488618237, w1=0.9755989470503743\n",
      "Gradient Descent(337/999): loss=9.806892571994064, w0=-1.735309837739158, w1=0.9759908939268442\n",
      "Gradient Descent(338/999): loss=9.80382029596522, w0=-1.7392042938089451, w1=0.9763821342815124\n",
      "Gradient Descent(339/999): loss=9.800759086097333, w0=-1.7430917297484947, w1=0.9767726693879522\n",
      "Gradient Descent(340/999): loss=9.797708902530731, w0=-1.746972158212264, w1=0.9771625005174409\n",
      "Gradient Descent(341/999): loss=9.794669705549314, w0=-1.7508455918318995, w1=0.9775516289389647\n",
      "Gradient Descent(342/999): loss=9.791641455580034, w0=-1.7547120432162782, w1=0.977940055919222\n",
      "Gradient Descent(343/999): loss=9.788624113192384, w0=-1.7585715249515481, w1=0.9783277827226277\n",
      "Gradient Descent(344/999): loss=9.78561763909789, w0=-1.7624240496011696, w1=0.9787148106113177\n",
      "Gradient Descent(345/999): loss=9.78262199414958, w0=-1.7662696297059564, w1=0.9791011408451529\n",
      "Gradient Descent(346/999): loss=9.7796371393415, w0=-1.7701082777841157, w1=0.979486774681723\n",
      "Gradient Descent(347/999): loss=9.77666303580818, w0=-1.7739400063312898, w1=0.9798717133763506\n",
      "Gradient Descent(348/999): loss=9.773699644824163, w0=-1.7777648278205964, w1=0.9802559581820959\n",
      "Gradient Descent(349/999): loss=9.77074692780345, w0=-1.7815827547026692, w1=0.98063951034976\n",
      "Gradient Descent(350/999): loss=9.767804846299049, w0=-1.7853937994056983, w1=0.9810223711278895\n",
      "Gradient Descent(351/999): loss=9.76487336200244, w0=-1.789197974335471, w1=0.98140454176278\n",
      "Gradient Descent(352/999): loss=9.761952436743098, w0=-1.7929952918754115, w1=0.9817860234984812\n",
      "Gradient Descent(353/999): loss=9.759042032487972, w0=-1.7967857643866225, w1=0.9821668175767997\n",
      "Gradient Descent(354/999): loss=9.756142111341017, w0=-1.800569404207924, w1=0.9825469252373037\n",
      "Gradient Descent(355/999): loss=9.753252635542685, w0=-1.8043462236558945, w1=0.9829263477173273\n",
      "Gradient Descent(356/999): loss=9.750373567469428, w0=-1.80811623502491, w1=0.983305086251974\n",
      "Gradient Descent(357/999): loss=9.74750486963322, w0=-1.8118794505871856, w1=0.9836831420741207\n",
      "Gradient Descent(358/999): loss=9.744646504681068, w0=-1.8156358825928138, w1=0.9840605164144222\n",
      "Gradient Descent(359/999): loss=9.741798435394523, w0=-1.8193855432698058, w1=0.9844372105013148\n",
      "Gradient Descent(360/999): loss=9.738960624689188, w0=-1.82312844482413, w1=0.9848132255610205\n",
      "Gradient Descent(361/999): loss=9.736133035614245, w0=-1.826864599439753, w1=0.9851885628175506\n",
      "Gradient Descent(362/999): loss=9.733315631351974, w0=-1.830594019278678, w1=0.9855632234927107\n",
      "Gradient Descent(363/999): loss=9.730508375217264, w0=-1.8343167164809855, w1=0.9859372088061034\n",
      "Gradient Descent(364/999): loss=9.727711230657148, w0=-1.8380327031648722, w1=0.9863105199751329\n",
      "Gradient Descent(365/999): loss=9.724924161250316, w0=-1.8417419914266904, w1=0.986683158215009\n",
      "Gradient Descent(366/999): loss=9.72214713070665, w0=-1.8454445933409878, w1=0.987055124738751\n",
      "Gradient Descent(367/999): loss=9.71938010286674, w0=-1.8491405209605465, w1=0.9874264207571918\n",
      "Gradient Descent(368/999): loss=9.71662304170143, w0=-1.8528297863164225, w1=0.9877970474789811\n",
      "Gradient Descent(369/999): loss=9.713875911311327, w0=-1.8565124014179843, w1=0.9881670061105906\n",
      "Gradient Descent(370/999): loss=9.711138675926355, w0=-1.8601883782529525, w1=0.9885362978563167\n",
      "Gradient Descent(371/999): loss=9.70841129990528, w0=-1.8638577287874387, w1=0.988904923918285\n",
      "Gradient Descent(372/999): loss=9.705693747735234, w0=-1.8675204649659847, w1=0.9892728854964544\n",
      "Gradient Descent(373/999): loss=9.702985984031278, w0=-1.8711765987116007, w1=0.9896401837886206\n",
      "Gradient Descent(374/999): loss=9.700287973535922, w0=-1.8748261419258045, w1=0.9900068199904203\n",
      "Gradient Descent(375/999): loss=9.697599681118671, w0=-1.878469106488661, w1=0.9903727952953345\n",
      "Gradient Descent(376/999): loss=9.694921071775568, w0=-1.882105504258819, w1=0.9907381108946934\n",
      "Gradient Descent(377/999): loss=9.692252110628738, w0=-1.885735347073552, w1=0.9911027679776796\n",
      "Gradient Descent(378/999): loss=9.689592762925937, w0=-1.8893586467487953, w1=0.9914667677313317\n",
      "Gradient Descent(379/999): loss=9.686942994040091, w0=-1.8929754150791847, w1=0.9918301113405489\n",
      "Gradient Descent(380/999): loss=9.684302769468857, w0=-1.896585663838095, w1=0.9921927999880944\n",
      "Gradient Descent(381/999): loss=9.681672054834163, w0=-1.9001894047776786, w1=0.9925548348545994\n",
      "Gradient Descent(382/999): loss=9.67905081588177, w0=-1.9037866496289035, w1=0.9929162171185668\n",
      "Gradient Descent(383/999): loss=9.67643901848081, w0=-1.9073774101015915, w1=0.9932769479563754\n",
      "Gradient Descent(384/999): loss=9.673836628623365, w0=-1.910961697884456, w1=0.9936370285422832\n",
      "Gradient Descent(385/999): loss=9.671243612424005, w0=-1.9145395246451407, w1=0.9939964600484315\n",
      "Gradient Descent(386/999): loss=9.668659936119354, w0=-1.9181109020302571, w1=0.9943552436448486\n",
      "Gradient Descent(387/999): loss=9.666085566067652, w0=-1.921675841665423, w1=0.9947133804994541\n",
      "Gradient Descent(388/999): loss=9.663520468748313, w0=-1.9252343551552993, w1=0.9950708717780621\n",
      "Gradient Descent(389/999): loss=9.660964610761486, w0=-1.9287864540836286, w1=0.9954277186443851\n",
      "Gradient Descent(390/999): loss=9.658417958827634, w0=-1.9323321500132729, w1=0.995783922260038\n",
      "Gradient Descent(391/999): loss=9.655880479787081, w0=-1.9358714544862508, w1=0.9961394837845419\n",
      "Gradient Descent(392/999): loss=9.653352140599598, w0=-1.9394043790237754, w1=0.9964944043753273\n",
      "Gradient Descent(393/999): loss=9.650832908343958, w0=-1.9429309351262916, w1=0.9968486851877391\n",
      "Gradient Descent(394/999): loss=9.64832275021752, w0=-1.9464511342735138, w1=0.9972023273750387\n",
      "Gradient Descent(395/999): loss=9.6458216335358, w0=-1.9499649879244632, w1=0.9975553320884093\n",
      "Gradient Descent(396/999): loss=9.643329525732025, w0=-1.9534725075175046, w1=0.9979077004769586\n",
      "Gradient Descent(397/999): loss=9.640846394356743, w0=-1.9569737044703845, w1=0.9982594336877233\n",
      "Gradient Descent(398/999): loss=9.638372207077374, w0=-1.9604685901802676, w1=0.998610532865672\n",
      "Gradient Descent(399/999): loss=9.635906931677805, w0=-1.9639571760237742, w1=0.9989609991537096\n",
      "Gradient Descent(400/999): loss=9.633450536057953, w0=-1.9674394733570169, w1=0.999310833692681\n",
      "Gradient Descent(401/999): loss=9.631002988233371, w0=-1.970915493515638, w1=0.9996600376213742\n",
      "Gradient Descent(402/999): loss=9.628564256334807, w0=-1.9743852478148467, w1=1.0000086120765248\n",
      "Gradient Descent(403/999): loss=9.626134308607803, w0=-1.9778487475494546, w1=1.000356558192819\n",
      "Gradient Descent(404/999): loss=9.62371311341228, w0=-1.9813060039939139, w1=1.0007038771028982\n",
      "Gradient Descent(405/999): loss=9.621300639222131, w0=-1.984757028402353, w1=1.0010505699373613\n",
      "Gradient Descent(406/999): loss=9.61889685462479, w0=-1.9882018320086143, w1=1.0013966378247696\n",
      "Gradient Descent(407/999): loss=9.61650172832085, w0=-1.9916404260262899, w1=1.0017420818916503\n",
      "Gradient Descent(408/999): loss=9.614115229123634, w0=-1.9950728216487579, w1=1.0020869032624995\n",
      "Gradient Descent(409/999): loss=9.611737325958806, w0=-1.9984990300492198, w1=1.0024311030597866\n",
      "Gradient Descent(410/999): loss=9.609367987863951, w0=-2.001919062380736, w1=1.0027746824039572\n",
      "Gradient Descent(411/999): loss=9.607007183988182, w0=-2.0053329297762628, w1=1.0031176424134378\n",
      "Gradient Descent(412/999): loss=9.604654883591731, w0=-2.008740643348688, w1=1.0034599842046386\n",
      "Gradient Descent(413/999): loss=9.602311056045561, w0=-2.0121422141908676, w1=1.0038017088919569\n",
      "Gradient Descent(414/999): loss=9.59997567083095, w0=-2.015537653375661, w1=1.004142817587782\n",
      "Gradient Descent(415/999): loss=9.59764869753911, w0=-2.018926971955968, w1=1.0044833114024974\n",
      "Gradient Descent(416/999): loss=9.595330105870783, w0=-2.0223101809647654, w1=1.004823191444485\n",
      "Gradient Descent(417/999): loss=9.593019865635837, w0=-2.025687291415141, w1=1.0051624588201293\n",
      "Gradient Descent(418/999): loss=9.590717946752898, w0=-2.0290583143003307, w1=1.00550111463382\n",
      "Gradient Descent(419/999): loss=9.588424319248933, w0=-2.0324232605937538, w1=1.005839159987956\n",
      "Gradient Descent(420/999): loss=9.586138953258875, w0=-2.0357821412490495, w1=1.0061765959829492\n",
      "Gradient Descent(421/999): loss=9.583861819025223, w0=-2.039134967200112, w1=1.006513423717228\n",
      "Gradient Descent(422/999): loss=9.581592886897674, w0=-2.042481749361125, w1=1.0068496442872403\n",
      "Gradient Descent(423/999): loss=9.579332127332709, w0=-2.0458224986266003, w1=1.0071852587874581\n",
      "Gradient Descent(424/999): loss=9.577079510893231, w0=-2.0491572258714092, w1=1.0075202683103803\n",
      "Gradient Descent(425/999): loss=9.57483500824817, w0=-2.0524859419508217, w1=1.0078546739465364\n",
      "Gradient Descent(426/999): loss=9.57259859017211, w0=-2.055808657700539, w1=1.0081884767844902\n",
      "Gradient Descent(427/999): loss=9.570370227544895, w0=-2.0591253839367307, w1=1.008521677910843\n",
      "Gradient Descent(428/999): loss=9.568149891351268, w0=-2.0624361314560686, w1=1.008854278410238\n",
      "Gradient Descent(429/999): loss=9.56593755268048, w0=-2.0657409110357627, w1=1.0091862793653623\n",
      "Gradient Descent(430/999): loss=9.563733182725908, w0=-2.069039733433596, w1=1.0095176818569525\n",
      "Gradient Descent(431/999): loss=9.561536752784704, w0=-2.07233260938796, w1=1.0098484869637963\n",
      "Gradient Descent(432/999): loss=9.55934823425739, w0=-2.075619549617888, w1=1.0101786957627366\n",
      "Gradient Descent(433/999): loss=9.557167598647517, w0=-2.078900564823093, w1=1.0105083093286757\n",
      "Gradient Descent(434/999): loss=9.554994817561267, w0=-2.082175665684, w1=1.0108373287345784\n",
      "Gradient Descent(435/999): loss=9.552829862707103, w0=-2.08544486286178, w1=1.0111657550514748\n",
      "Gradient Descent(436/999): loss=9.550672705895384, w0=-2.0887081669983885, w1=1.0114935893484647\n",
      "Gradient Descent(437/999): loss=9.548523319038011, w0=-2.091965588716597, w1=1.011820832692721\n",
      "Gradient Descent(438/999): loss=9.546381674148062, w0=-2.095217138620028, w1=1.0121474861494921\n",
      "Gradient Descent(439/999): loss=9.544247743339415, w0=-2.09846282729319, w1=1.0124735507821072\n",
      "Gradient Descent(440/999): loss=9.542121498826392, w0=-2.1017026653015125, w1=1.012799027651978\n",
      "Gradient Descent(441/999): loss=9.540002912923402, w0=-2.1049366631913795, w1=1.0131239178186033\n",
      "Gradient Descent(442/999): loss=9.537891958044572, w0=-2.108164831490164, w1=1.0134482223395718\n",
      "Gradient Descent(443/999): loss=9.535788606703395, w0=-2.111387180706263, w1=1.013771942270566\n",
      "Gradient Descent(444/999): loss=9.533692831512367, w0=-2.11460372132913, w1=1.014095078665365\n",
      "Gradient Descent(445/999): loss=9.53160460518263, w0=-2.117814463829311, w1=1.0144176325758494\n",
      "Gradient Descent(446/999): loss=9.529523900523625, w0=-2.121019418658478, w1=1.0147396050520026\n",
      "Gradient Descent(447/999): loss=9.527450690442725, w0=-2.1242185962494626, w1=1.0150609971419156\n",
      "Gradient Descent(448/999): loss=9.525384947944893, w0=-2.1274120070162903, w1=1.0153818098917904\n",
      "Gradient Descent(449/999): loss=9.523326646132329, w0=-2.1305996613542137, w1=1.0157020443459428\n",
      "Gradient Descent(450/999): loss=9.521275758204109, w0=-2.1337815696397477, w1=1.0160217015468063\n",
      "Gradient Descent(451/999): loss=9.519232257455851, w0=-2.1369577422307025, w1=1.0163407825349353\n",
      "Gradient Descent(452/999): loss=9.51719611727936, w0=-2.140128189466217, w1=1.0166592883490084\n",
      "Gradient Descent(453/999): loss=9.51516731116228, w0=-2.1432929216667933, w1=1.016977220025832\n",
      "Gradient Descent(454/999): loss=9.513145812687753, w0=-2.1464519491343292, w1=1.0172945786003436\n",
      "Gradient Descent(455/999): loss=9.511131595534074, w0=-2.149605282152153, w1=1.017611365105615\n",
      "Gradient Descent(456/999): loss=9.509124633474347, w0=-2.1527529309850553, w1=1.0179275805728554\n",
      "Gradient Descent(457/999): loss=9.507124900376134, w0=-2.155894905879325, w1=1.018243226031416\n",
      "Gradient Descent(458/999): loss=9.50513237020114, w0=-2.159031217062779, w1=1.0185583025087923\n",
      "Gradient Descent(459/999): loss=9.50314701700485, w0=-2.1621618747448, w1=1.0188728110306269\n",
      "Gradient Descent(460/999): loss=9.501168814936197, w0=-2.165286889116365, w1=1.0191867526207141\n",
      "Gradient Descent(461/999): loss=9.499197738237239, w0=-2.1684062703500824, w1=1.0195001283010032\n",
      "Gradient Descent(462/999): loss=9.4972337612428, w0=-2.1715200286002228, w1=1.0198129390916004\n",
      "Gradient Descent(463/999): loss=9.495276858380155, w0=-2.174628174002753, w1=1.0201251860107736\n",
      "Gradient Descent(464/999): loss=9.493327004168691, w0=-2.1777307166753683, w1=1.0204368700749549\n",
      "Gradient Descent(465/999): loss=9.491384173219572, w0=-2.180827666717527, w1=1.0207479922987448\n",
      "Gradient Descent(466/999): loss=9.489448340235413, w0=-2.1839190342104806, w1=1.021058553694914\n",
      "Gradient Descent(467/999): loss=9.487519480009947, w0=-2.1870048292173094, w1=1.0213685552744083\n",
      "Gradient Descent(468/999): loss=9.4855975674277, w0=-2.1900850617829537, w1=1.0216779980463508\n",
      "Gradient Descent(469/999): loss=9.483682577463666, w0=-2.1931597419342466, w1=1.0219868830180456\n",
      "Gradient Descent(470/999): loss=9.481774485182966, w0=-2.1962288796799467, w1=1.0222952111949812\n",
      "Gradient Descent(471/999): loss=9.479873265740547, w0=-2.1992924850107713, w1=1.0226029835808335\n",
      "Gradient Descent(472/999): loss=9.477978894380843, w0=-2.2023505678994284, w1=1.0229102011774691\n",
      "Gradient Descent(473/999): loss=9.476091346437455, w0=-2.205403138300649, w1=1.023216864984949\n",
      "Gradient Descent(474/999): loss=9.474210597332831, w0=-2.2084502061512206, w1=1.0235229760015305\n",
      "Gradient Descent(475/999): loss=9.472336622577945, w0=-2.2114917813700172, w1=1.0238285352236731\n",
      "Gradient Descent(476/999): loss=9.470469397771978, w0=-2.2145278738580343, w1=1.0241335436460386\n",
      "Gradient Descent(477/999): loss=9.46860889860201, w0=-2.2175584934984194, w1=1.0244380022614963\n",
      "Gradient Descent(478/999): loss=9.466755100842683, w0=-2.220583650156505, w1=1.024741912061126\n",
      "Gradient Descent(479/999): loss=9.464907980355903, w0=-2.22360335367984, w1=1.025045274034221\n",
      "Gradient Descent(480/999): loss=9.463067513090524, w0=-2.226617613898222, w1=1.0253480891682911\n",
      "Gradient Descent(481/999): loss=9.46123367508202, w0=-2.22962644062373, w1=1.0256503584490657\n",
      "Gradient Descent(482/999): loss=9.459406442452195, w0=-2.2326298436507557, w1=1.025952082860498\n",
      "Gradient Descent(483/999): loss=9.457585791408855, w0=-2.2356278327560353, w1=1.026253263384767\n",
      "Gradient Descent(484/999): loss=9.455771698245503, w0=-2.238620417698681, w1=1.0265539010022815\n",
      "Gradient Descent(485/999): loss=9.453964139341036, w0=-2.2416076082202143, w1=1.0268539966916828\n",
      "Gradient Descent(486/999): loss=9.452163091159434, w0=-2.2445894140445963, w1=1.0271535514298484\n",
      "Gradient Descent(487/999): loss=9.450368530249442, w0=-2.247565844878259, w1=1.027452566191895\n",
      "Gradient Descent(488/999): loss=9.448580433244285, w0=-2.2505369104101387, w1=1.0277510419511808\n",
      "Gradient Descent(489/999): loss=9.446798776861353, w0=-2.2535026203117057, w1=1.0280489796793102\n",
      "Gradient Descent(490/999): loss=9.445023537901893, w0=-2.256462984236997, w1=1.0283463803461357\n",
      "Gradient Descent(491/999): loss=9.443254693250719, w0=-2.2594180118226475, w1=1.0286432449197622\n",
      "Gradient Descent(492/999): loss=9.441492219875899, w0=-2.26236771268792, w1=1.0289395743665486\n",
      "Gradient Descent(493/999): loss=9.439736094828465, w0=-2.2653120964347386, w1=1.0292353696511127\n",
      "Gradient Descent(494/999): loss=9.437986295242107, w0=-2.268251172647719, w1=1.0295306317363329\n",
      "Gradient Descent(495/999): loss=9.43624279833288, w0=-2.2711849508941993, w1=1.029825361583352\n",
      "Gradient Descent(496/999): loss=9.434505581398904, w0=-2.2741134407242716, w1=1.0301195601515802\n",
      "Gradient Descent(497/999): loss=9.432774621820071, w0=-2.2770366516708136, w1=1.0304132283986989\n",
      "Gradient Descent(498/999): loss=9.43104989705775, w0=-2.2799545932495184, w1=1.0307063672806624\n",
      "Gradient Descent(499/999): loss=9.429331384654489, w0=-2.2828672749589267, w1=1.0309989777517024\n",
      "Gradient Descent(500/999): loss=9.427619062233731, w0=-2.2857747062804568, w1=1.0312910607643297\n",
      "Gradient Descent(501/999): loss=9.42591290749952, w0=-2.288676896678436, w1=1.0315826172693392\n",
      "Gradient Descent(502/999): loss=9.424212898236195, w0=-2.2915738556001313, w1=1.0318736482158113\n",
      "Gradient Descent(503/999): loss=9.422519012308134, w0=-2.2944655924757797, w1=1.032164154551115\n",
      "Gradient Descent(504/999): loss=9.420831227659432, w0=-2.2973521167186197, w1=1.0324541372209128\n",
      "Gradient Descent(505/999): loss=9.419149522313635, w0=-2.3002334377249216, w1=1.0327435971691619\n",
      "Gradient Descent(506/999): loss=9.417473874373442, w0=-2.303109564874018, w1=1.0330325353381178\n",
      "Gradient Descent(507/999): loss=9.415804262020432, w0=-2.3059805075283335, w1=1.0333209526683376\n",
      "Gradient Descent(508/999): loss=9.414140663514765, w0=-2.3088462750334173, w1=1.033608850098683\n",
      "Gradient Descent(509/999): loss=9.412483057194908, w0=-2.3117068767179716, w1=1.0338962285663238\n",
      "Gradient Descent(510/999): loss=9.41083142147736, w0=-2.3145623218938827, w1=1.0341830890067396\n",
      "Gradient Descent(511/999): loss=9.40918573485635, w0=-2.3174126198562517, w1=1.0344694323537242\n",
      "Gradient Descent(512/999): loss=9.407545975903579, w0=-2.3202577798834243, w1=1.034755259539388\n",
      "Gradient Descent(513/999): loss=9.405912123267926, w0=-2.3230978112370213, w1=1.0350405714941615\n",
      "Gradient Descent(514/999): loss=9.404284155675178, w0=-2.325932723161968, w1=1.0353253691467974\n",
      "Gradient Descent(515/999): loss=9.402662051927752, w0=-2.3287625248865247, w1=1.0356096534243748\n",
      "Gradient Descent(516/999): loss=9.401045790904416, w0=-2.3315872256223176, w1=1.0358934252523015\n",
      "Gradient Descent(517/999): loss=9.399435351560008, w0=-2.334406834564368, w1=1.0361766855543169\n",
      "Gradient Descent(518/999): loss=9.397830712925185, w0=-2.3372213608911214, w1=1.0364594352524954\n",
      "Gradient Descent(519/999): loss=9.396231854106123, w0=-2.3400308137644794, w1=1.0367416752672496\n",
      "Gradient Descent(520/999): loss=9.394638754284255, w0=-2.3428352023298276, w1=1.0370234065173325\n",
      "Gradient Descent(521/999): loss=9.393051392716014, w0=-2.3456345357160666, w1=1.0373046299198414\n",
      "Gradient Descent(522/999): loss=9.391469748732533, w0=-2.348428823035641, w1=1.03758534639022\n",
      "Gradient Descent(523/999): loss=9.389893801739406, w0=-2.35121807338457, w1=1.0378655568422621\n",
      "Gradient Descent(524/999): loss=9.3883235312164, w0=-2.354002295842475, w1=1.0381452621881142\n",
      "Gradient Descent(525/999): loss=9.386758916717202, w0=-2.356781499472612, w1=1.0384244633382789\n",
      "Gradient Descent(526/999): loss=9.385199937869135, w0=-2.359555693321899, w1=1.0387031612016169\n",
      "Gradient Descent(527/999): loss=9.383646574372914, w0=-2.3623248864209456, w1=1.038981356685351\n",
      "Gradient Descent(528/999): loss=9.382098806002363, w0=-2.3650890877840833, w1=1.0392590506950687\n",
      "Gradient Descent(529/999): loss=9.380556612604162, w0=-2.3678483064093947, w1=1.039536244134725\n",
      "Gradient Descent(530/999): loss=9.379019974097583, w0=-2.370602551278742, w1=1.039812937906645\n",
      "Gradient Descent(531/999): loss=9.377488870474224, w0=-2.3733518313577973, w1=1.040089132911528\n",
      "Gradient Descent(532/999): loss=9.37596328179776, w0=-2.3760961555960702, w1=1.0403648300484494\n",
      "Gradient Descent(533/999): loss=9.374443188203669, w0=-2.378835532926939, w1=1.0406400302148637\n",
      "Gradient Descent(534/999): loss=9.37292856989898, w0=-2.381569972267678, w1=1.0409147343066083\n",
      "Gradient Descent(535/999): loss=9.371419407162016, w0=-2.3842994825194883, w1=1.041188943217905\n",
      "Gradient Descent(536/999): loss=9.369915680342139, w0=-2.3870240725675242, w1=1.0414626578413637\n",
      "Gradient Descent(537/999): loss=9.36841736985949, w0=-2.3897437512809248, w1=1.0417358790679863\n",
      "Gradient Descent(538/999): loss=9.366924456204732, w0=-2.392458527512841, w1=1.0420086077871673\n",
      "Gradient Descent(539/999): loss=9.365436919938805, w0=-2.395168410100466, w1=1.0422808448866987\n",
      "Gradient Descent(540/999): loss=9.363954741692663, w0=-2.3978734078650623, w1=1.042552591252772\n",
      "Gradient Descent(541/999): loss=9.362477902167026, w0=-2.4005735296119917, w1=1.0428238477699816\n",
      "Gradient Descent(542/999): loss=9.361006382132135, w0=-2.4032687841307427, w1=1.0430946153213267\n",
      "Gradient Descent(543/999): loss=9.35954016242749, w0=-2.405959180194961, w1=1.0433648947882153\n",
      "Gradient Descent(544/999): loss=9.358079223961607, w0=-2.408644726562476, w1=1.043634687050466\n",
      "Gradient Descent(545/999): loss=9.35662354771177, w0=-2.4113254319753312, w1=1.0439039929863125\n",
      "Gradient Descent(546/999): loss=9.355173114723776, w0=-2.414001305159811, w1=1.0441728134724042\n",
      "Gradient Descent(547/999): loss=9.3537279061117, w0=-2.41667235482647, w1=1.044441149383811\n",
      "Gradient Descent(548/999): loss=9.352287903057649, w0=-2.419338589670162, w1=1.0447090015940252\n",
      "Gradient Descent(549/999): loss=9.350853086811494, w0=-2.4220000183700656, w1=1.0449763709749644\n",
      "Gradient Descent(550/999): loss=9.349423438690657, w0=-2.4246566495897164, w1=1.045243258396975\n",
      "Gradient Descent(551/999): loss=9.347998940079847, w0=-2.4273084919770316, w1=1.045509664728834\n",
      "Gradient Descent(552/999): loss=9.346579572430826, w0=-2.4299555541643407, w1=1.0457755908377522\n",
      "Gradient Descent(553/999): loss=9.345165317262166, w0=-2.4325978447684125, w1=1.0460410375893778\n",
      "Gradient Descent(554/999): loss=9.34375615615901, w0=-2.4352353723904825, w1=1.046306005847798\n",
      "Gradient Descent(555/999): loss=9.342352070772826, w0=-2.4378681456162825, w1=1.046570496475543\n",
      "Gradient Descent(556/999): loss=9.340953042821173, w0=-2.4404961730160672, w1=1.0468345103335874\n",
      "Gradient Descent(557/999): loss=9.339559054087466, w0=-2.443119463144643, w1=1.0470980482813548\n",
      "Gradient Descent(558/999): loss=9.338170086420732, w0=-2.4457380245413947, w1=1.0473611111767187\n",
      "Gradient Descent(559/999): loss=9.336786121735377, w0=-2.4483518657303147, w1=1.0476236998760067\n",
      "Gradient Descent(560/999): loss=9.33540714201095, w0=-2.45096099522003, w1=1.0478858152340025\n",
      "Gradient Descent(561/999): loss=9.334033129291914, w0=-2.4535654215038303, w1=1.0481474581039494\n",
      "Gradient Descent(562/999): loss=9.332664065687396, w0=-2.456165153059694, w1=1.0484086293375523\n",
      "Gradient Descent(563/999): loss=9.331299933370977, w0=-2.458760198350319, w1=1.0486693297849807\n",
      "Gradient Descent(564/999): loss=9.329940714580438, w0=-2.461350565823147, w1=1.048929560294872\n",
      "Gradient Descent(565/999): loss=9.328586391617542, w0=-2.4639362639103926, w1=1.0491893217143338\n",
      "Gradient Descent(566/999): loss=9.327236946847803, w0=-2.466517301029071, w1=1.0494486148889461\n",
      "Gradient Descent(567/999): loss=9.325892362700248, w0=-2.469093685581025, w1=1.0497074406627656\n",
      "Gradient Descent(568/999): loss=9.324552621667198, w0=-2.471665425952951, w1=1.0499657998783265\n",
      "Gradient Descent(569/999): loss=9.323217706304035, w0=-2.474232530516429, w1=1.0502236933766451\n",
      "Gradient Descent(570/999): loss=9.321887599228972, w0=-2.4767950076279486, w1=1.0504811219972212\n",
      "Gradient Descent(571/999): loss=9.320562283122834, w0=-2.4793528656289343, w1=1.0507380865780418\n",
      "Gradient Descent(572/999): loss=9.31924174072883, w0=-2.4819061128457762, w1=1.0509945879555826\n",
      "Gradient Descent(573/999): loss=9.31792595485232, w0=-2.4844547575898543, w1=1.0512506269648123\n",
      "Gradient Descent(574/999): loss=9.316614908360604, w0=-2.4869988081575665, w1=1.0515062044391936\n",
      "Gradient Descent(575/999): loss=9.315308584182691, w0=-2.489538272830356, w1=1.0517613212106878\n",
      "Gradient Descent(576/999): loss=9.314006965309083, w0=-2.4920731598747383, w1=1.0520159781097556\n",
      "Gradient Descent(577/999): loss=9.31271003479154, w0=-2.4946034775423267, w1=1.0522701759653612\n",
      "Gradient Descent(578/999): loss=9.311417775742878, w0=-2.497129234069861, w1=1.0525239156049746\n",
      "Gradient Descent(579/999): loss=9.310130171336734, w0=-2.4996504376792332, w1=1.0527771978545741\n",
      "Gradient Descent(580/999): loss=9.308847204807355, w0=-2.5021670965775145, w1=1.0530300235386487\n",
      "Gradient Descent(581/999): loss=9.307568859449376, w0=-2.504679218956982, w1=1.0532823934802016\n",
      "Gradient Descent(582/999): loss=9.306295118617602, w0=-2.507186812995146, w1=1.0535343085007525\n",
      "Gradient Descent(583/999): loss=9.305025965726799, w0=-2.509689886854775, w1=1.05378576942034\n",
      "Gradient Descent(584/999): loss=9.303761384251468, w0=-2.512188448683924, w1=1.0540367770575247\n",
      "Gradient Descent(585/999): loss=9.302501357725632, w0=-2.514682506615961, w1=1.0542873322293915\n",
      "Gradient Descent(586/999): loss=9.30124586974263, w0=-2.5171720687695913, w1=1.0545374357515525\n",
      "Gradient Descent(587/999): loss=9.299994903954886, w0=-2.5196571432488866, w1=1.0547870884381496\n",
      "Gradient Descent(588/999): loss=9.298748444073718, w0=-2.52213773814331, w1=1.055036291101857\n",
      "Gradient Descent(589/999): loss=9.29750647386911, w0=-2.524613861527742, w1=1.0552850445538842\n",
      "Gradient Descent(590/999): loss=9.296268977169508, w0=-2.5270855214625088, w1=1.055533349603978\n",
      "Gradient Descent(591/999): loss=9.295035937861602, w0=-2.529552725993405, w1=1.0557812070604262\n",
      "Gradient Descent(592/999): loss=9.293807339890124, w0=-2.532015483151724, w1=1.0560286177300593\n",
      "Gradient Descent(593/999): loss=9.29258316725764, w0=-2.53447380095428, w1=1.056275582418253\n",
      "Gradient Descent(594/999): loss=9.291363404024331, w0=-2.5369276874034377, w1=1.056522101928932\n",
      "Gradient Descent(595/999): loss=9.290148034307798, w0=-2.5393771504871365, w1=1.056768177064571\n",
      "Gradient Descent(596/999): loss=9.288937042282846, w0=-2.541822198178916, w1=1.0570138086261986\n",
      "Gradient Descent(597/999): loss=9.287730412181283, w0=-2.5442628384379438, w1=1.0572589974133997\n",
      "Gradient Descent(598/999): loss=9.28652812829171, w0=-2.546699079209039, w1=1.0575037442243178\n",
      "Gradient Descent(599/999): loss=9.285330174959327, w0=-2.549130928422701, w1=1.057748049855657\n",
      "Gradient Descent(600/999): loss=9.284136536585713, w0=-2.5515583939951316, w1=1.0579919151026862\n",
      "Gradient Descent(601/999): loss=9.282947197628637, w0=-2.5539814838282653, w1=1.0582353407592402\n",
      "Gradient Descent(602/999): loss=9.281762142601849, w0=-2.556400205809791, w1=1.0584783276177228\n",
      "Gradient Descent(603/999): loss=9.28058135607488, w0=-2.55881456781318, w1=1.0587208764691094\n",
      "Gradient Descent(604/999): loss=9.279404822672838, w0=-2.5612245776977107, w1=1.0589629881029503\n",
      "Gradient Descent(605/999): loss=9.278232527076213, w0=-2.563630243308494, w1=1.0592046633073717\n",
      "Gradient Descent(606/999): loss=9.277064454020676, w0=-2.5660315724765, w1=1.0594459028690795\n",
      "Gradient Descent(607/999): loss=9.275900588296878, w0=-2.5684285730185823, w1=1.0596867075733616\n",
      "Gradient Descent(608/999): loss=9.27474091475025, w0=-2.570821252737504, w1=1.0599270782040897\n",
      "Gradient Descent(609/999): loss=9.273585418280813, w0=-2.573209619421962, w1=1.0601670155437235\n",
      "Gradient Descent(610/999): loss=9.272434083842976, w0=-2.575593680846615, w1=1.0604065203733115\n",
      "Gradient Descent(611/999): loss=9.271286896445343, w0=-2.5779734447721063, w1=1.0606455934724943\n",
      "Gradient Descent(612/999): loss=9.270143841150512, w0=-2.58034891894509, w1=1.0608842356195078\n",
      "Gradient Descent(613/999): loss=9.269004903074887, w0=-2.582720111098256, w1=1.0611224475911842\n",
      "Gradient Descent(614/999): loss=9.267870067388486, w0=-2.585087028950355, w1=1.0613602301629559\n",
      "Gradient Descent(615/999): loss=9.266739319314734, w0=-2.5874496802062246, w1=1.0615975841088572\n",
      "Gradient Descent(616/999): loss=9.265612644130293, w0=-2.589808072556813, w1=1.0618345102015276\n",
      "Gradient Descent(617/999): loss=9.264490027164843, w0=-2.5921622136792055, w1=1.0620710092122134\n",
      "Gradient Descent(618/999): loss=9.263371453800922, w0=-2.594512111236648, w1=1.0623070819107707\n",
      "Gradient Descent(619/999): loss=9.262256909473702, w0=-2.5968577728785727, w1=1.0625427290656682\n",
      "Gradient Descent(620/999): loss=9.26114637967083, w0=-2.5991992062406233, w1=1.0627779514439888\n",
      "Gradient Descent(621/999): loss=9.260039849932223, w0=-2.6015364189446797, w1=1.0630127498114332\n",
      "Gradient Descent(622/999): loss=9.258937305849873, w0=-2.603869418598882, w1=1.0632471249323217\n",
      "Gradient Descent(623/999): loss=9.257838733067684, w0=-2.606198212797657, w1=1.0634810775695966\n",
      "Gradient Descent(624/999): loss=9.256744117281258, w0=-2.6085228091217405, w1=1.063714608484825\n",
      "Gradient Descent(625/999): loss=9.255653444237726, w0=-2.6108432151382037, w1=1.0639477184382018\n",
      "Gradient Descent(626/999): loss=9.254566699735555, w0=-2.613159438400478, w1=1.0641804081885504\n",
      "Gradient Descent(627/999): loss=9.253483869624365, w0=-2.615471486448379, w1=1.0644126784933277\n",
      "Gradient Descent(628/999): loss=9.252404939804748, w0=-2.6177793668081297, w1=1.064644530108624\n",
      "Gradient Descent(629/999): loss=9.251329896228075, w0=-2.620083086992388, w1=1.0648759637891674\n",
      "Gradient Descent(630/999): loss=9.250258724896327, w0=-2.6223826545002686, w1=1.0651069802883253\n",
      "Gradient Descent(631/999): loss=9.2491914118619, w0=-2.6246780768173688, w1=1.0653375803581073\n",
      "Gradient Descent(632/999): loss=9.248127943227427, w0=-2.626969361415792, w1=1.065567764749167\n",
      "Gradient Descent(633/999): loss=9.247068305145607, w0=-2.6292565157541725, w1=1.0657975342108053\n",
      "Gradient Descent(634/999): loss=9.246012483819005, w0=-2.6315395472777, w1=1.0660268894909721\n",
      "Gradient Descent(635/999): loss=9.24496046549989, w0=-2.6338184634181436, w1=1.0662558313362696\n",
      "Gradient Descent(636/999): loss=9.24391223649005, w0=-2.636093271593875, w1=1.0664843604919532\n",
      "Gradient Descent(637/999): loss=9.242867783140614, w0=-2.638363979209895, w1=1.0667124777019357\n",
      "Gradient Descent(638/999): loss=9.241827091851865, w0=-2.6406305936578547, w1=1.0669401837087886\n",
      "Gradient Descent(639/999): loss=9.240790149073081, w0=-2.642893122316082, w1=1.0671674792537453\n",
      "Gradient Descent(640/999): loss=9.239756941302346, w0=-2.6451515725496044, w1=1.0673943650767024\n",
      "Gradient Descent(641/999): loss=9.238727455086376, w0=-2.6474059517101733, w1=1.067620841916223\n",
      "Gradient Descent(642/999): loss=9.237701677020343, w0=-2.6496562671362875, w1=1.0678469105095392\n",
      "Gradient Descent(643/999): loss=9.23667959374771, w0=-2.651902526153218, w1=1.0680725715925536\n",
      "Gradient Descent(644/999): loss=9.235661191960041, w0=-2.654144736073031, w1=1.0682978258998428\n",
      "Gradient Descent(645/999): loss=9.234646458396838, w0=-2.6563829041946123, w1=1.068522674164659\n",
      "Gradient Descent(646/999): loss=9.23363537984537, w0=-2.65861703780369, w1=1.0687471171189327\n",
      "Gradient Descent(647/999): loss=9.2326279431405, w0=-2.66084714417286, w1=1.0689711554932748\n",
      "Gradient Descent(648/999): loss=9.2316241351645, w0=-2.6630732305616074, w1=1.0691947900169796\n",
      "Gradient Descent(649/999): loss=9.2306239428469, w0=-2.665295304216333, w1=1.0694180214180262\n",
      "Gradient Descent(650/999): loss=9.229627353164311, w0=-2.6675133723703737, w1=1.0696408504230823\n",
      "Gradient Descent(651/999): loss=9.228634353140244, w0=-2.6697274422440285, w1=1.069863277757505\n",
      "Gradient Descent(652/999): loss=9.22764492984496, w0=-2.671937521044581, w1=1.070085304145344\n",
      "Gradient Descent(653/999): loss=9.226659070395284, w0=-2.674143615966323, w1=1.0703069303093442\n",
      "Gradient Descent(654/999): loss=9.225676761954455, w0=-2.676345734190578, w1=1.0705281569709473\n",
      "Gradient Descent(655/999): loss=9.22469799173194, w0=-2.6785438828857235, w1=1.0707489848502947\n",
      "Gradient Descent(656/999): loss=9.223722746983285, w0=-2.6807380692072167, w1=1.0709694146662294\n",
      "Gradient Descent(657/999): loss=9.222751015009933, w0=-2.6829283002976156, w1=1.071189447136299\n",
      "Gradient Descent(658/999): loss=9.221782783159071, w0=-2.6851145832866035, w1=1.0714090829767575\n",
      "Gradient Descent(659/999): loss=9.220818038823461, w0=-2.687296925291011, w1=1.0716283229025678\n",
      "Gradient Descent(660/999): loss=9.219856769441272, w0=-2.6894753334148405, w1=1.071847167627404\n",
      "Gradient Descent(661/999): loss=9.218898962495924, w0=-2.691649814749289, w1=1.0720656178636536\n",
      "Gradient Descent(662/999): loss=9.21794460551592, w0=-2.6938203763727704, w1=1.0722836743224204\n",
      "Gradient Descent(663/999): loss=9.216993686074682, w0=-2.6959870253509397, w1=1.0725013377135257\n",
      "Gradient Descent(664/999): loss=9.216046191790394, w0=-2.6981497687367146, w1=1.0727186087455116\n",
      "Gradient Descent(665/999): loss=9.21510211032584, w0=-2.7003086135702996, w1=1.0729354881256434\n",
      "Gradient Descent(666/999): loss=9.214161429388238, w0=-2.7024635668792087, w1=1.0731519765599107\n",
      "Gradient Descent(667/999): loss=9.213224136729089, w0=-2.7046146356782885, w1=1.073368074753031\n",
      "Gradient Descent(668/999): loss=9.21229022014401, w0=-2.7067618269697395, w1=1.0735837834084514\n",
      "Gradient Descent(669/999): loss=9.211359667472571, w0=-2.708905147743141, w1=1.0737991032283511\n",
      "Gradient Descent(670/999): loss=9.210432466598153, w0=-2.7110446049754726, w1=1.0740140349136431\n",
      "Gradient Descent(671/999): loss=9.20950860544778, w0=-2.7131802056311374, w1=1.0742285791639772\n",
      "Gradient Descent(672/999): loss=9.208588071991953, w0=-2.715311956661984, w1=1.0744427366777423\n",
      "Gradient Descent(673/999): loss=9.207670854244512, w0=-2.717439865007331, w1=1.074656508152068\n",
      "Gradient Descent(674/999): loss=9.206756940262466, w0=-2.719563937593986, w1=1.0748698942828272\n",
      "Gradient Descent(675/999): loss=9.20584631814584, w0=-2.7216841813362724, w1=1.075082895764639\n",
      "Gradient Descent(676/999): loss=9.20493897603753, w0=-2.723800603136049, w1=1.0752955132908697\n",
      "Gradient Descent(677/999): loss=9.20403490212313, w0=-2.725913209882733, w1=1.0755077475536359\n",
      "Gradient Descent(678/999): loss=9.203134084630795, w0=-2.728022008453323, w1=1.0757195992438071\n",
      "Gradient Descent(679/999): loss=9.202236511831085, w0=-2.730127005712422, w1=1.0759310690510067\n",
      "Gradient Descent(680/999): loss=9.2013421720368, w0=-2.732228208512258, w1=1.0761421576636157\n",
      "Gradient Descent(681/999): loss=9.200451053602844, w0=-2.734325623692707, w1=1.0763528657687735\n",
      "Gradient Descent(682/999): loss=9.199563144926058, w0=-2.7364192580813165, w1=1.0765631940523814\n",
      "Gradient Descent(683/999): loss=9.198678434445089, w0=-2.7385091184933255, w1=1.0767731431991043\n",
      "Gradient Descent(684/999): loss=9.197796910640214, w0=-2.740595211731689, w1=1.0769827138923727\n",
      "Gradient Descent(685/999): loss=9.196918562033215, w0=-2.742677544587098, w1=1.077191906814385\n",
      "Gradient Descent(686/999): loss=9.196043377187207, w0=-2.7447561238380036, w1=1.0774007226461102\n",
      "Gradient Descent(687/999): loss=9.195171344706509, w0=-2.746830956250637, w1=1.0776091620672898\n",
      "Gradient Descent(688/999): loss=9.194302453236482, w0=-2.7489020485790334, w1=1.0778172257564398\n",
      "Gradient Descent(689/999): loss=9.19343669146339, w0=-2.7509694075650533, w1=1.0780249143908531\n",
      "Gradient Descent(690/999): loss=9.192574048114238, w0=-2.7530330399384035, w1=1.078232228646602\n",
      "Gradient Descent(691/999): loss=9.19171451195665, w0=-2.755092952416661, w1=1.0784391691985398\n",
      "Gradient Descent(692/999): loss=9.190858071798697, w0=-2.757149151705293, w1=1.0786457367203033\n",
      "Gradient Descent(693/999): loss=9.190004716488769, w0=-2.7592016444976792, w1=1.0788519318843153\n",
      "Gradient Descent(694/999): loss=9.189154434915423, w0=-2.7612504374751348, w1=1.0790577553617862\n",
      "Gradient Descent(695/999): loss=9.188307216007232, w0=-2.7632955373069303, w1=1.0792632078227167\n",
      "Gradient Descent(696/999): loss=9.18746304873266, w0=-2.7653369506503154, w1=1.0794682899358996\n",
      "Gradient Descent(697/999): loss=9.186621922099892, w0=-2.767374684150538, w1=1.0796730023689225\n",
      "Gradient Descent(698/999): loss=9.185783825156717, w0=-2.769408744440868, w1=1.079877345788169\n",
      "Gradient Descent(699/999): loss=9.184948746990361, w0=-2.771439138142618, w1=1.0800813208588216\n",
      "Gradient Descent(700/999): loss=9.184116676727372, w0=-2.773465871865166, w1=1.0802849282448643\n",
      "Gradient Descent(701/999): loss=9.183287603533453, w0=-2.775488952205975, w1=1.080488168609084\n",
      "Gradient Descent(702/999): loss=9.182461516613333, w0=-2.7775083857506155, w1=1.080691042613072\n",
      "Gradient Descent(703/999): loss=9.181638405210625, w0=-2.7795241790727867, w1=1.0808935509172284\n",
      "Gradient Descent(704/999): loss=9.18081825860769, w0=-2.781536338734339, w1=1.081095694180762\n",
      "Gradient Descent(705/999): loss=9.180001066125488, w0=-2.7835448712852933, w1=1.0812974730616933\n",
      "Gradient Descent(706/999): loss=9.179186817123448, w0=-2.7855497832638645, w1=1.0814988882168572\n",
      "Gradient Descent(707/999): loss=9.178375500999323, w0=-2.787551081196481, w1=1.0816999403019039\n",
      "Gradient Descent(708/999): loss=9.177567107189056, w0=-2.7895487715978073, w1=1.0819006299713023\n",
      "Gradient Descent(709/999): loss=9.176761625166643, w0=-2.7915428609707638, w1=1.0821009578783414\n",
      "Gradient Descent(710/999): loss=9.175959044443992, w0=-2.793533355806549, w1=1.082300924675132\n",
      "Gradient Descent(711/999): loss=9.175159354570786, w0=-2.7955202625846614, w1=1.0825005310126106\n",
      "Gradient Descent(712/999): loss=9.174362545134354, w0=-2.7975035877729177, w1=1.0826997775405394\n",
      "Gradient Descent(713/999): loss=9.173568605759531, w0=-2.7994833378274775, w1=1.0828986649075094\n",
      "Gradient Descent(714/999): loss=9.172777526108518, w0=-2.801459519192862, w1=1.0830971937609428\n",
      "Gradient Descent(715/999): loss=9.171989295880753, w0=-2.8034321383019747, w1=1.0832953647470946\n",
      "Gradient Descent(716/999): loss=9.171203904812783, w0=-2.8054012015761245, w1=1.0834931785110549\n",
      "Gradient Descent(717/999): loss=9.170421342678114, w0=-2.8073667154250446, w1=1.0836906356967508\n",
      "Gradient Descent(718/999): loss=9.169641599287093, w0=-2.809328686246914, w1=1.0838877369469488\n",
      "Gradient Descent(719/999): loss=9.16886466448677, w0=-2.811287120428378, w1=1.084084482903257\n",
      "Gradient Descent(720/999): loss=9.16809052816076, w0=-2.81324202434457, w1=1.0842808742061265\n",
      "Gradient Descent(721/999): loss=9.167319180229123, w0=-2.8151934043591322, w1=1.084476911494854\n",
      "Gradient Descent(722/999): loss=9.166550610648223, w0=-2.817141266824234, w1=1.0846725954075842\n",
      "Gradient Descent(723/999): loss=9.165784809410598, w0=-2.819085618080596, w1=1.0848679265813108\n",
      "Gradient Descent(724/999): loss=9.165021766544836, w0=-2.821026464457508, w1=1.0850629056518801\n",
      "Gradient Descent(725/999): loss=9.164261472115445, w0=-2.822963812272851, w1=1.0852575332539913\n",
      "Gradient Descent(726/999): loss=9.16350391622271, w0=-2.8248976678331177, w1=1.0854518100212005\n",
      "Gradient Descent(727/999): loss=9.162749089002585, w0=-2.8268280374334327, w1=1.0856457365859207\n",
      "Gradient Descent(728/999): loss=9.161996980626546, w0=-2.8287549273575725, w1=1.0858393135794258\n",
      "Gradient Descent(729/999): loss=9.161247581301472, w0=-2.830678343877987, w1=1.0860325416318513\n",
      "Gradient Descent(730/999): loss=9.16050088126952, w0=-2.8325982932558187, w1=1.086225421372197\n",
      "Gradient Descent(731/999): loss=9.159756870807993, w0=-2.8345147817409253, w1=1.0864179534283285\n",
      "Gradient Descent(732/999): loss=9.159015540229213, w0=-2.8364278155718967, w1=1.0866101384269804\n",
      "Gradient Descent(733/999): loss=9.158276879880397, w0=-2.8383374009760787, w1=1.0868019769937567\n",
      "Gradient Descent(734/999): loss=9.157540880143527, w0=-2.8402435441695904, w1=1.0869934697531343\n",
      "Gradient Descent(735/999): loss=9.156807531435238, w0=-2.842146251357347, w1=1.0871846173284638\n",
      "Gradient Descent(736/999): loss=9.156076824206673, w0=-2.8440455287330777, w1=1.087375420341973\n",
      "Gradient Descent(737/999): loss=9.155348748943378, w0=-2.845941382479347, w1=1.0875658794147671\n",
      "Gradient Descent(738/999): loss=9.154623296165166, w0=-2.847833818767576, w1=1.0877559951668327\n",
      "Gradient Descent(739/999): loss=9.153900456425994, w0=-2.8497228437580597, w1=1.087945768217038\n",
      "Gradient Descent(740/999): loss=9.15318022031385, w0=-2.851608463599989, w1=1.0881351991831358\n",
      "Gradient Descent(741/999): loss=9.152462578450615, w0=-2.853490684431471, w1=1.088324288681766\n",
      "Gradient Descent(742/999): loss=9.15174752149196, w0=-2.855369512379547, w1=1.088513037328456\n",
      "Gradient Descent(743/999): loss=9.151035040127201, w0=-2.857244953560215, w1=1.088701445737624\n",
      "Gradient Descent(744/999): loss=9.150325125079204, w0=-2.8591170140784476, w1=1.0888895145225812\n",
      "Gradient Descent(745/999): loss=9.14961776710424, w0=-2.8609857000282126, w1=1.0890772442955325\n",
      "Gradient Descent(746/999): loss=9.148912956991882, w0=-2.8628510174924933, w1=1.0892646356675795\n",
      "Gradient Descent(747/999): loss=9.148210685564877, w0=-2.8647129725433076, w1=1.0894516892487223\n",
      "Gradient Descent(748/999): loss=9.147510943679022, w0=-2.8665715712417277, w1=1.0896384056478612\n",
      "Gradient Descent(749/999): loss=9.146813722223058, w0=-2.8684268196379006, w1=1.0898247854727994\n",
      "Gradient Descent(750/999): loss=9.146119012118543, w0=-2.870278723771067, w1=1.090010829330244\n",
      "Gradient Descent(751/999): loss=9.145426804319733, w0=-2.8721272896695815, w1=1.0901965378258085\n",
      "Gradient Descent(752/999): loss=9.144737089813468, w0=-2.873972523350932, w1=1.090381911564015\n",
      "Gradient Descent(753/999): loss=9.144049859619052, w0=-2.8758144308217592, w1=1.0905669511482956\n",
      "Gradient Descent(754/999): loss=9.143365104788135, w0=-2.8776530180778765, w1=1.0907516571809948\n",
      "Gradient Descent(755/999): loss=9.142682816404605, w0=-2.8794882911042885, w1=1.0909360302633713\n",
      "Gradient Descent(756/999): loss=9.142002985584458, w0=-2.8813202558752122, w1=1.0911200709956\n",
      "Gradient Descent(757/999): loss=9.141325603475691, w0=-2.8831489183540953, w1=1.0913037799767735\n",
      "Gradient Descent(758/999): loss=9.140650661258189, w0=-2.884974284493635, w1=1.0914871578049055\n",
      "Gradient Descent(759/999): loss=9.139978150143605, w0=-2.8867963602357993, w1=1.091670205076931\n",
      "Gradient Descent(760/999): loss=9.139308061375246, w0=-2.888615151511845, w1=1.0918529223887083\n",
      "Gradient Descent(761/999): loss=9.138640386227962, w0=-2.8904306642423365, w1=1.0920353103350233\n",
      "Gradient Descent(762/999): loss=9.137975116008024, w0=-2.8922429043371665, w1=1.0922173695095885\n",
      "Gradient Descent(763/999): loss=9.137312242053028, w0=-2.894051877695574, w1=1.0923991005050462\n",
      "Gradient Descent(764/999): loss=9.136651755731762, w0=-2.895857590206165, w1=1.0925805039129708\n",
      "Gradient Descent(765/999): loss=9.13599364844411, w0=-2.89766004774693, w1=1.0927615803238704\n",
      "Gradient Descent(766/999): loss=9.135337911620931, w0=-2.899459256185264, w1=1.0929423303271883\n",
      "Gradient Descent(767/999): loss=9.134684536723947, w0=-2.901255221377985, w1=1.0931227545113058\n",
      "Gradient Descent(768/999): loss=9.134033515245633, w0=-2.903047949171355, w1=1.0933028534635427\n",
      "Gradient Descent(769/999): loss=9.13338483870912, w0=-2.9048374454010957, w1=1.0934826277701608\n",
      "Gradient Descent(770/999): loss=9.13273849866805, w0=-2.9066237158924104, w1=1.0936620780163653\n",
      "Gradient Descent(771/999): loss=9.132094486706514, w0=-2.908406766460002, w1=1.0938412047863058\n",
      "Gradient Descent(772/999): loss=9.131452794438896, w0=-2.910186602908091, w1=1.0940200086630791\n",
      "Gradient Descent(773/999): loss=9.130813413509793, w0=-2.911963231030436, w1=1.0941984902287314\n",
      "Gradient Descent(774/999): loss=9.130176335593902, w0=-2.913736656610352, w1=1.0943766500642593\n",
      "Gradient Descent(775/999): loss=9.129541552395896, w0=-2.915506885420728, w1=1.0945544887496121\n",
      "Gradient Descent(776/999): loss=9.128909055650338, w0=-2.9172739232240477, w1=1.094732006863694\n",
      "Gradient Descent(777/999): loss=9.128278837121556, w0=-2.919037775772407, w1=1.0949092049843652\n",
      "Gradient Descent(778/999): loss=9.127650888603544, w0=-2.9207984488075334, w1=1.0950860836884444\n",
      "Gradient Descent(779/999): loss=9.127025201919857, w0=-2.9225559480608037, w1=1.0952626435517108\n",
      "Gradient Descent(780/999): loss=9.126401768923497, w0=-2.9243102792532643, w1=1.0954388851489054\n",
      "Gradient Descent(781/999): loss=9.125780581496812, w0=-2.926061448095648, w1=1.0956148090537334\n",
      "Gradient Descent(782/999): loss=9.12516163155139, w0=-2.927809460288394, w1=1.0957904158388658\n",
      "Gradient Descent(783/999): loss=9.12454491102795, w0=-2.9295543215216657, w1=1.0959657060759411\n",
      "Gradient Descent(784/999): loss=9.123930411896247, w0=-2.9312960374753696, w1=1.0961406803355673\n",
      "Gradient Descent(785/999): loss=9.123318126154956, w0=-2.9330346138191734, w1=1.0963153391873246\n",
      "Gradient Descent(786/999): loss=9.122708045831569, w0=-2.934770056212525, w1=1.0964896831997653\n",
      "Gradient Descent(787/999): loss=9.1221001629823, w0=-2.9365023703046704, w1=1.0966637129404178\n",
      "Gradient Descent(788/999): loss=9.121494469691974, w0=-2.938231561734672, w1=1.096837428975787\n",
      "Gradient Descent(789/999): loss=9.120890958073922, w0=-2.9399576361314277, w1=1.0970108318713567\n",
      "Gradient Descent(790/999): loss=9.120289620269894, w0=-2.9416805991136883, w1=1.0971839221915916\n",
      "Gradient Descent(791/999): loss=9.119690448449933, w0=-2.943400456290077, w1=1.0973567004999385\n",
      "Gradient Descent(792/999): loss=9.119093434812289, w0=-2.945117213259106, w1=1.0975291673588288\n",
      "Gradient Descent(793/999): loss=9.118498571583316, w0=-2.946830875609197, w1=1.0977013233296797\n",
      "Gradient Descent(794/999): loss=9.117905851017362, w0=-2.9485414489186965, w1=1.097873168972897\n",
      "Gradient Descent(795/999): loss=9.117315265396684, w0=-2.950248938755896, w1=1.0980447048478763\n",
      "Gradient Descent(796/999): loss=9.116726807031329, w0=-2.95195335067905, w1=1.0982159315130038\n",
      "Gradient Descent(797/999): loss=9.116140468259045, w0=-2.9536546902363936, w1=1.0983868495256603\n",
      "Gradient Descent(798/999): loss=9.115556241445184, w0=-2.9553529629661606, w1=1.0985574594422214\n",
      "Gradient Descent(799/999): loss=9.114974118982591, w0=-2.9570481743966015, w1=1.09872776181806\n",
      "Gradient Descent(800/999): loss=9.114394093291512, w0=-2.958740330046002, w1=1.0988977572075473\n",
      "Gradient Descent(801/999): loss=9.113816156819503, w0=-2.960429435422699, w1=1.0990674461640562\n",
      "Gradient Descent(802/999): loss=9.113240302041312, w0=-2.962115496025103, w1=1.0992368292399608\n",
      "Gradient Descent(803/999): loss=9.112666521458799, w0=-2.9637985173417105, w1=1.0994059069866409\n",
      "Gradient Descent(804/999): loss=9.112094807600833, w0=-2.9654785048511254, w1=1.099574679954481\n",
      "Gradient Descent(805/999): loss=9.11152515302319, w0=-2.967155464022076, w1=1.0997431486928748\n",
      "Gradient Descent(806/999): loss=9.110957550308461, w0=-2.9688294003134326, w1=1.0999113137502248\n",
      "Gradient Descent(807/999): loss=9.110391992065956, w0=-2.970500319174225, w1=1.100079175673945\n",
      "Gradient Descent(808/999): loss=9.109828470931603, w0=-2.9721682260436615, w1=1.1002467350104628\n",
      "Gradient Descent(809/999): loss=9.109266979567854, w0=-2.9738331263511446, w1=1.100413992305221\n",
      "Gradient Descent(810/999): loss=9.108707510663596, w0=-2.975495025516291, w1=1.1005809481026785\n",
      "Gradient Descent(811/999): loss=9.108150056934043, w0=-2.9771539289489466, w1=1.100747602946313\n",
      "Gradient Descent(812/999): loss=9.107594611120653, w0=-2.9788098420492064, w1=1.100913957378623\n",
      "Gradient Descent(813/999): loss=9.10704116599103, w0=-2.9804627702074313, w1=1.1010800119411284\n",
      "Gradient Descent(814/999): loss=9.106489714338826, w0=-2.9821127188042653, w1=1.1012457671743734\n",
      "Gradient Descent(815/999): loss=9.105940248983647, w0=-2.9837596932106534, w1=1.1014112236179274\n",
      "Gradient Descent(816/999): loss=9.10539276277097, w0=-2.9854036987878585, w1=1.1015763818103879\n",
      "Gradient Descent(817/999): loss=9.104847248572035, w0=-2.98704474088748, w1=1.1017412422893806\n",
      "Gradient Descent(818/999): loss=9.104303699283765, w0=-2.98868282485147, w1=1.101905805591563\n",
      "Gradient Descent(819/999): loss=9.103762107828661, w0=-2.990317956012152, w1=1.1020700722526242\n",
      "Gradient Descent(820/999): loss=9.103222467154726, w0=-2.991950139692236, w1=1.1022340428072888\n",
      "Gradient Descent(821/999): loss=9.102684770235356, w0=-2.993579381204839, w1=1.1023977177893167\n",
      "Gradient Descent(822/999): loss=9.102149010069258, w0=-2.9952056858534992, w1=1.102561097731506\n",
      "Gradient Descent(823/999): loss=9.101615179680362, w0=-2.9968290589321955, w1=1.1027241831656942\n",
      "Gradient Descent(824/999): loss=9.10108327211772, w0=-2.998449505725364, w1=1.10288697462276\n",
      "Gradient Descent(825/999): loss=9.100553280455422, w0=-3.0000670315079145, w1=1.1030494726326259\n",
      "Gradient Descent(826/999): loss=9.100025197792505, w0=-3.0016816415452485, w1=1.1032116777242582\n",
      "Gradient Descent(827/999): loss=9.099499017252866, w0=-3.003293341093276, w1=1.1033735904256698\n",
      "Gradient Descent(828/999): loss=9.098974731985166, w0=-3.0049021353984333, w1=1.1035352112639227\n",
      "Gradient Descent(829/999): loss=9.098452335162744, w0=-3.0065080296976987, w1=1.103696540765128\n",
      "Gradient Descent(830/999): loss=9.097931819983527, w0=-3.0081110292186106, w1=1.1038575794544485\n",
      "Gradient Descent(831/999): loss=9.097413179669948, w0=-3.009711139179285, w1=1.1040183278561004\n",
      "Gradient Descent(832/999): loss=9.09689640746885, w0=-3.0113083647884302, w1=1.1041787864933554\n",
      "Gradient Descent(833/999): loss=9.096381496651397, w0=-3.012902711245367, w1=1.1043389558885413\n",
      "Gradient Descent(834/999): loss=9.095868440512994, w0=-3.0144941837400427, w1=1.1044988365630448\n",
      "Gradient Descent(835/999): loss=9.095357232373193, w0=-3.01608278745305, w1=1.1046584290373125\n",
      "Gradient Descent(836/999): loss=9.094847865575609, w0=-3.017668527555642, w1=1.104817733830853\n",
      "Gradient Descent(837/999): loss=9.094340333487835, w0=-3.0192514092097515, w1=1.1049767514622382\n",
      "Gradient Descent(838/999): loss=9.093834629501355, w0=-3.0208314375680057, w1=1.1051354824491055\n",
      "Gradient Descent(839/999): loss=9.093330747031448, w0=-3.0224086177737437, w1=1.1052939273081595\n",
      "Gradient Descent(840/999): loss=9.092828679517119, w0=-3.0239829549610335, w1=1.1054520865551722\n",
      "Gradient Descent(841/999): loss=9.092328420421005, w0=-3.025554454254688, w1=1.1056099607049876\n",
      "Gradient Descent(842/999): loss=9.091829963229284, w0=-3.027123120770283, w1=1.10576755027152\n",
      "Gradient Descent(843/999): loss=9.091333301451602, w0=-3.028688959614172, w1=1.1059248557677583\n",
      "Gradient Descent(844/999): loss=9.09083842862098, w0=-3.030251975883504, w1=1.1060818777057666\n",
      "Gradient Descent(845/999): loss=9.090345338293737, w0=-3.03181217466624, w1=1.1062386165966853\n",
      "Gradient Descent(846/999): loss=9.089854024049398, w0=-3.03336956104117, w1=1.1063950729507344\n",
      "Gradient Descent(847/999): loss=9.089364479490612, w0=-3.0349241400779285, w1=1.1065512472772134\n",
      "Gradient Descent(848/999): loss=9.088876698243077, w0=-3.0364759168370115, w1=1.106707140084504\n",
      "Gradient Descent(849/999): loss=9.088390673955443, w0=-3.0380248963697927, w1=1.1068627518800718\n",
      "Gradient Descent(850/999): loss=9.087906400299248, w0=-3.039571083718541, w1=1.1070180831704672\n",
      "Gradient Descent(851/999): loss=9.087423870968813, w0=-3.0411144839164357, w1=1.1071731344613274\n",
      "Gradient Descent(852/999): loss=9.086943079681184, w0=-3.0426551019875827, w1=1.1073279062573786\n",
      "Gradient Descent(853/999): loss=9.086464020176024, w0=-3.0441929429470327, w1=1.1074823990624372\n",
      "Gradient Descent(854/999): loss=9.085986686215556, w0=-3.045728011800795, w1=1.1076366133794109\n",
      "Gradient Descent(855/999): loss=9.085511071584468, w0=-3.0472603135458565, w1=1.107790549710301\n",
      "Gradient Descent(856/999): loss=9.085037170089834, w0=-3.048789853170195, w1=1.1079442085562048\n",
      "Gradient Descent(857/999): loss=9.084564975561033, w0=-3.0503166356527984, w1=1.1080975904173147\n",
      "Gradient Descent(858/999): loss=9.084094481849677, w0=-3.0518406659636788, w1=1.1082506957929228\n",
      "Gradient Descent(859/999): loss=9.083625682829517, w0=-3.053361949063889, w1=1.1084035251814206\n",
      "Gradient Descent(860/999): loss=9.083158572396371, w0=-3.05488048990554, w1=1.1085560790803013\n",
      "Gradient Descent(861/999): loss=9.082693144468053, w0=-3.056396293431815, w1=1.1087083579861612\n",
      "Gradient Descent(862/999): loss=9.08222939298427, w0=-3.0579093645769877, w1=1.108860362394702\n",
      "Gradient Descent(863/999): loss=9.081767311906571, w0=-3.0594197082664367, w1=1.109012092800731\n",
      "Gradient Descent(864/999): loss=9.081306895218248, w0=-3.0609273294166623, w1=1.109163549698164\n",
      "Gradient Descent(865/999): loss=9.080848136924267, w0=-3.0624322329353024, w1=1.1093147335800269\n",
      "Gradient Descent(866/999): loss=9.080391031051184, w0=-3.0639344237211485, w1=1.109465644938456\n",
      "Gradient Descent(867/999): loss=9.079935571647075, w0=-3.065433906664161, w1=1.1096162842647008\n",
      "Gradient Descent(868/999): loss=9.079481752781458, w0=-3.066930686645487, w1=1.1097666520491256\n",
      "Gradient Descent(869/999): loss=9.079029568545206, w0=-3.0684247685374726, w1=1.1099167487812107\n",
      "Gradient Descent(870/999): loss=9.078579013050478, w0=-3.069916157203683, w1=1.1100665749495537\n",
      "Gradient Descent(871/999): loss=9.07813008043064, w0=-3.071404857498916, w1=1.1102161310418717\n",
      "Gradient Descent(872/999): loss=9.07768276484019, w0=-3.0728908742692176, w1=1.1103654175450026\n",
      "Gradient Descent(873/999): loss=9.077237060454685, w0=-3.0743742123518984, w1=1.110514434944907\n",
      "Gradient Descent(874/999): loss=9.076792961470652, w0=-3.07585487657555, w1=1.1106631837266687\n",
      "Gradient Descent(875/999): loss=9.076350462105534, w0=-3.0773328717600594, w1=1.1108116643744983\n",
      "Gradient Descent(876/999): loss=9.07590955659759, w0=-3.0788082027166253, w1=1.1109598773717324\n",
      "Gradient Descent(877/999): loss=9.075470239205846, w0=-3.080280874247774, w1=1.1111078232008371\n",
      "Gradient Descent(878/999): loss=9.07503250420999, w0=-3.081750891147374, w1=1.1112555023434083\n",
      "Gradient Descent(879/999): loss=9.07459634591033, w0=-3.0832182582006538, w1=1.1114029152801743\n",
      "Gradient Descent(880/999): loss=9.074161758627696, w0=-3.084682980184215, w1=1.1115500624909966\n",
      "Gradient Descent(881/999): loss=9.073728736703373, w0=-3.0861450618660493, w1=1.1116969444548714\n",
      "Gradient Descent(882/999): loss=9.073297274499032, w0=-3.0876045080055534, w1=1.111843561649932\n",
      "Gradient Descent(883/999): loss=9.072867366396652, w0=-3.0890613233535453, w1=1.1119899145534495\n",
      "Gradient Descent(884/999): loss=9.072439006798447, w0=-3.0905155126522783, w1=1.1121360036418346\n",
      "Gradient Descent(885/999): loss=9.072012190126793, w0=-3.091967080635458, w1=1.1122818293906394\n",
      "Gradient Descent(886/999): loss=9.071586910824161, w0=-3.0934160320282573, w1=1.112427392274559\n",
      "Gradient Descent(887/999): loss=9.071163163353035, w0=-3.0948623715473302, w1=1.112572692767432\n",
      "Gradient Descent(888/999): loss=9.070740942195844, w0=-3.09630610390083, w1=1.1127177313422438\n",
      "Gradient Descent(889/999): loss=9.070320241854898, w0=-3.097747233788422, w1=1.1128625084711268\n",
      "Gradient Descent(890/999): loss=9.0699010568523, w0=-3.099185765901301, w1=1.113007024625362\n",
      "Gradient Descent(891/999): loss=9.069483381729892, w0=-3.1006217049222045, w1=1.1131512802753818\n",
      "Gradient Descent(892/999): loss=9.069067211049168, w0=-3.102055055525429, w1=1.1132952758907695\n",
      "Gradient Descent(893/999): loss=9.068652539391222, w0=-3.103485822376846, w1=1.1134390119402626\n",
      "Gradient Descent(894/999): loss=9.068239361356657, w0=-3.104914010133915, w1=1.1135824888917536\n",
      "Gradient Descent(895/999): loss=9.067827671565528, w0=-3.106339623445701, w1=1.1137257072122915\n",
      "Gradient Descent(896/999): loss=9.067417464657268, w0=-3.1077626669528886, w1=1.113868667368083\n",
      "Gradient Descent(897/999): loss=9.06700873529062, w0=-3.1091831452877967, w1=1.1140113698244951\n",
      "Gradient Descent(898/999): loss=9.066601478143564, w0=-3.110601063074394, w1=1.1141538150460557\n",
      "Gradient Descent(899/999): loss=9.066195687913247, w0=-3.1120164249283144, w1=1.114296003496455\n",
      "Gradient Descent(900/999): loss=9.065791359315924, w0=-3.113429235456871, w1=1.114437935638548\n",
      "Gradient Descent(901/999): loss=9.065388487086874, w0=-3.1148394992590727, w1=1.1145796119343547\n",
      "Gradient Descent(902/999): loss=9.064987065980343, w0=-3.1162472209256373, w1=1.1147210328450625\n",
      "Gradient Descent(903/999): loss=9.064587090769471, w0=-3.1176524050390086, w1=1.1148621988310277\n",
      "Gradient Descent(904/999): loss=9.06418855624623, w0=-3.119055056173369, w1=1.1150031103517763\n",
      "Gradient Descent(905/999): loss=9.06379145722134, w0=-3.1204551788946557, w1=1.1151437678660066\n",
      "Gradient Descent(906/999): loss=9.063395788524222, w0=-3.1218527777605756, w1=1.1152841718315896\n",
      "Gradient Descent(907/999): loss=9.063001545002916, w0=-3.12324785732062, w1=1.1154243227055707\n",
      "Gradient Descent(908/999): loss=9.062608721524025, w0=-3.124640422116079, w1=1.115564220944172\n",
      "Gradient Descent(909/999): loss=9.062217312972635, w0=-3.126030476680057, w1=1.1157038670027932\n",
      "Gradient Descent(910/999): loss=9.061827314252259, w0=-3.1274180255374864, w1=1.1158432613360127\n",
      "Gradient Descent(911/999): loss=9.061438720284766, w0=-3.1288030732051437, w1=1.1159824043975894\n",
      "Gradient Descent(912/999): loss=9.061051526010319, w0=-3.130185624191663, w1=1.116121296640465\n",
      "Gradient Descent(913/999): loss=9.060665726387306, w0=-3.131565682997551, w1=1.1162599385167638\n",
      "Gradient Descent(914/999): loss=9.060281316392267, w0=-3.1329432541152027, w1=1.1163983304777958\n",
      "Gradient Descent(915/999): loss=9.059898291019849, w0=-3.134318342028914, w1=1.116536472974057\n",
      "Gradient Descent(916/999): loss=9.059516645282718, w0=-3.1356909512148983, w1=1.1166743664552319\n",
      "Gradient Descent(917/999): loss=9.059136374211509, w0=-3.1370610861412995, w1=1.1168120113701938\n",
      "Gradient Descent(918/999): loss=9.05875747285475, w0=-3.138428751268208, w1=1.116949408167007\n",
      "Gradient Descent(919/999): loss=9.058379936278815, w0=-3.139793951047673, w1=1.1170865572929285\n",
      "Gradient Descent(920/999): loss=9.058003759567837, w0=-3.141156689923721, w1=1.1172234591944088\n",
      "Gradient Descent(921/999): loss=9.057628937823665, w0=-3.142516972332365, w1=1.1173601143170935\n",
      "Gradient Descent(922/999): loss=9.057255466165785, w0=-3.1438748027016237, w1=1.1174965231058251\n",
      "Gradient Descent(923/999): loss=9.056883339731264, w0=-3.1452301854515325, w1=1.1176326860046442\n",
      "Gradient Descent(924/999): loss=9.056512553674683, w0=-3.14658312499416, w1=1.1177686034567906\n",
      "Gradient Descent(925/999): loss=9.056143103168083, w0=-3.147933625733622, w1=1.117904275904706\n",
      "Gradient Descent(926/999): loss=9.055774983400884, w0=-3.149281692066094, w1=1.1180397037900336\n",
      "Gradient Descent(927/999): loss=9.055408189579843, w0=-3.1506273283798283, w1=1.118174887553621\n",
      "Gradient Descent(928/999): loss=9.05504271692898, w0=-3.1519705390551667, w1=1.1183098276355212\n",
      "Gradient Descent(929/999): loss=9.054678560689513, w0=-3.153311328464554, w1=1.1184445244749934\n",
      "Gradient Descent(930/999): loss=9.054315716119806, w0=-3.154649700972555, w1=1.1185789785105058\n",
      "Gradient Descent(931/999): loss=9.0539541784953, w0=-3.155985660935866, w1=1.1187131901797356\n",
      "Gradient Descent(932/999): loss=9.053593943108453, w0=-3.1573192127033294, w1=1.1188471599195713\n",
      "Gradient Descent(933/999): loss=9.053235005268679, w0=-3.1586503606159493, w1=1.1189808881661139\n",
      "Gradient Descent(934/999): loss=9.052877360302293, w0=-3.1599791090069043, w1=1.119114375354678\n",
      "Gradient Descent(935/999): loss=9.052521003552435, w0=-3.1613054622015624, w1=1.1192476219197935\n",
      "Gradient Descent(936/999): loss=9.052165930379026, w0=-3.162629424517494, w1=1.119380628295208\n",
      "Gradient Descent(937/999): loss=9.051812136158695, w0=-3.1639510002644875, w1=1.1195133949138856\n",
      "Gradient Descent(938/999): loss=9.05145961628473, w0=-3.1652701937445618, w1=1.1196459222080113\n",
      "Gradient Descent(939/999): loss=9.051108366167005, w0=-3.1665870092519817, w1=1.1197782106089904\n",
      "Gradient Descent(940/999): loss=9.050758381231937, w0=-3.1679014510732704, w1=1.1199102605474507\n",
      "Gradient Descent(941/999): loss=9.050409656922405, w0=-3.1692135234872247, w1=1.1200420724532436\n",
      "Gradient Descent(942/999): loss=9.050062188697712, w0=-3.1705232307649283, w1=1.1201736467554462\n",
      "Gradient Descent(943/999): loss=9.049715972033512, w0=-3.171830577169766, w1=1.1203049838823613\n",
      "Gradient Descent(944/999): loss=9.049371002421758, w0=-3.1731355669574373, w1=1.1204360842615202\n",
      "Gradient Descent(945/999): loss=9.049027275370637, w0=-3.1744382043759707, w1=1.1205669483196834\n",
      "Gradient Descent(946/999): loss=9.048684786404518, w0=-3.1757384936657367, w1=1.1206975764828424\n",
      "Gradient Descent(947/999): loss=9.04834353106389, w0=-3.1770364390594623, w1=1.1208279691762202\n",
      "Gradient Descent(948/999): loss=9.048003504905306, w0=-3.178332044782245, w1=1.1209581268242739\n",
      "Gradient Descent(949/999): loss=9.047664703501322, w0=-3.179625315051566, w1=1.121088049850695\n",
      "Gradient Descent(950/999): loss=9.047327122440443, w0=-3.1809162540773035, w1=1.1212177386784117\n",
      "Gradient Descent(951/999): loss=9.046990757327066, w0=-3.1822048660617477, w1=1.1213471937295894\n",
      "Gradient Descent(952/999): loss=9.046655603781414, w0=-3.1834911551996132, w1=1.1214764154256327\n",
      "Gradient Descent(953/999): loss=9.046321657439492, w0=-3.184775125678054, w1=1.1216054041871868\n",
      "Gradient Descent(954/999): loss=9.045988913953023, w0=-3.1860567816766756, w1=1.1217341604341384\n",
      "Gradient Descent(955/999): loss=9.045657368989389, w0=-3.1873361273675496, w1=1.1218626845856172\n",
      "Gradient Descent(956/999): loss=9.045327018231578, w0=-3.1886131669152276, w1=1.1219909770599976\n",
      "Gradient Descent(957/999): loss=9.044997857378139, w0=-3.189887904476753, w1=1.1221190382748998\n",
      "Gradient Descent(958/999): loss=9.044669882143094, w0=-3.191160344201677, w1=1.1222468686471911\n",
      "Gradient Descent(959/999): loss=9.04434308825592, w0=-3.19243049023207, w1=1.1223744685929875\n",
      "Gradient Descent(960/999): loss=9.044017471461467, w0=-3.193698346702536, w1=1.1225018385276548\n",
      "Gradient Descent(961/999): loss=9.043693027519918, w0=-3.194963917740226, w1=1.1226289788658101\n",
      "Gradient Descent(962/999): loss=9.043369752206722, w0=-3.1962272074648523, w1=1.1227558900213233\n",
      "Gradient Descent(963/999): loss=9.04304764131255, w0=-3.1974882199887, w1=1.1228825724073177\n",
      "Gradient Descent(964/999): loss=9.042726690643228, w0=-3.1987469594166416, w1=1.1230090264361725\n",
      "Gradient Descent(965/999): loss=9.042406896019695, w0=-3.20000342984615, w1=1.1231352525195233\n",
      "Gradient Descent(966/999): loss=9.042088253277942, w0=-3.2012576353673126, w1=1.1232612510682636\n",
      "Gradient Descent(967/999): loss=9.041770758268957, w0=-3.2025095800628436, w1=1.1233870224925466\n",
      "Gradient Descent(968/999): loss=9.041454406858673, w0=-3.203759268008098, w1=1.1235125672017858\n",
      "Gradient Descent(969/999): loss=9.04113919492791, w0=-3.2050067032710845, w1=1.1236378856046567\n",
      "Gradient Descent(970/999): loss=9.04082511837233, w0=-3.2062518899124783, w1=1.123762978109098\n",
      "Gradient Descent(971/999): loss=9.040512173102382, w0=-3.207494831985636, w1=1.1238878451223138\n",
      "Gradient Descent(972/999): loss=9.040200355043233, w0=-3.2087355335366063, w1=1.1240124870507733\n",
      "Gradient Descent(973/999): loss=9.03988966013474, w0=-3.2099739986041453, w1=1.1241369043002134\n",
      "Gradient Descent(974/999): loss=9.039580084331376, w0=-3.2112102312197286, w1=1.1242610972756395\n",
      "Gradient Descent(975/999): loss=9.039271623602193, w0=-3.212444235407565, w1=1.124385066381327\n",
      "Gradient Descent(976/999): loss=9.038964273930754, w0=-3.213676015184609, w1=1.1245088120208229\n",
      "Gradient Descent(977/999): loss=9.038658031315094, w0=-3.214905574560574, w1=1.124632334596946\n",
      "Gradient Descent(978/999): loss=9.038352891767664, w0=-3.216132917537946, w1=1.1247556345117895\n",
      "Gradient Descent(979/999): loss=9.038048851315274, w0=-3.2173580481119957, w1=1.1248787121667216\n",
      "Gradient Descent(980/999): loss=9.037745905999047, w0=-3.218580970270792, w1=1.1250015679623875\n",
      "Gradient Descent(981/999): loss=9.037444051874367, w0=-3.219801687995215, w1=1.1251242022987094\n",
      "Gradient Descent(982/999): loss=9.037143285010824, w0=-3.221020205258969, w1=1.125246615574889\n",
      "Gradient Descent(983/999): loss=9.03684360149217, w0=-3.2222365260285954, w1=1.1253688081894087\n",
      "Gradient Descent(984/999): loss=9.03654499741625, w0=-3.223450654263485, w1=1.1254907805400318\n",
      "Gradient Descent(985/999): loss=9.036247468894981, w0=-3.2246625939158915, w1=1.1256125330238056\n",
      "Gradient Descent(986/999): loss=9.035951012054278, w0=-3.2258723489309453, w1=1.1257340660370607\n",
      "Gradient Descent(987/999): loss=9.035655623034003, w0=-3.227079923246664, w1=1.125855379975414\n",
      "Gradient Descent(988/999): loss=9.035361297987937, w0=-3.228285320793967, w1=1.125976475233769\n",
      "Gradient Descent(989/999): loss=9.0350680330837, w0=-3.2294885454966886, w1=1.1260973522063173\n",
      "Gradient Descent(990/999): loss=9.034775824502722, w0=-3.230689601271589, w1=1.12621801128654\n",
      "Gradient Descent(991/999): loss=9.034484668440191, w0=-3.2318884920283684, w1=1.126338452867209\n",
      "Gradient Descent(992/999): loss=9.034194561104997, w0=-3.2330852216696795, w1=1.126458677340388\n",
      "Gradient Descent(993/999): loss=9.03390549871968, w0=-3.23427979409114, w1=1.1265786850974338\n",
      "Gradient Descent(994/999): loss=9.033617477520393, w0=-3.235472213181345, w1=1.1266984765289982\n",
      "Gradient Descent(995/999): loss=9.033330493756845, w0=-3.236662482821881, w1=1.1268180520250288\n",
      "Gradient Descent(996/999): loss=9.033044543692252, w0=-3.2378506068873367, w1=1.12693741197477\n",
      "Gradient Descent(997/999): loss=9.03275962360329, w0=-3.2390365892453166, w1=1.1270565567667645\n",
      "Gradient Descent(998/999): loss=9.032475729780046, w0=-3.2402204337564537, w1=1.1271754867888548\n",
      "Gradient Descent(999/999): loss=9.032192858525969, w0=-3.241402144274422, w1=1.1272942024281842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.24140214,  1.1272942 ])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.031911006157825"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(y, X, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
