{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "\n",
    " Implement Linear Classification using pytorch. This consists of having fully connected layers connected one after the other and ReLu activation functions between them.\n",
    " \n",
    " Build a neural network with a minimun of 2 layers in order to do classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x127d32790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import pdb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(data_dir='../data',\n",
    "                           batch_size=64,\n",
    "                           augment=False,\n",
    "                           random_seed = 1,\n",
    "                           valid_size=0.02,\n",
    "                           shuffle=True,\n",
    "                           show_sample=False,\n",
    "                           num_workers=0,\n",
    "                           pin_memory=False):\n",
    "    \"\"\"\n",
    "    Utility function for loading and returning train and valid\n",
    "    multi-process iterators over the CIFAR-10 dataset. A sample\n",
    "    9x9 grid of the images can be optionally displayed.\n",
    "    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n",
    "    Params\n",
    "    ------\n",
    "    - data_dir: path directory to the dataset.\n",
    "    - batch_size: how many samples per batch to load.\n",
    "    - augment: whether to apply the data augmentation scheme\n",
    "      mentioned in the paper. Only applied on the train split.\n",
    "    - random_seed: fix seed for reproducibility.\n",
    "    - valid_size: percentage split of the training set used for\n",
    "      the validation set. Should be a float in the range [0, 1].\n",
    "    - shuffle: whether to shuffle the train/validation indices.\n",
    "    - show_sample: plot 9x9 sample grid of the dataset.\n",
    "    - num_workers: number of subprocesses to use when loading the dataset.\n",
    "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
    "      True if using GPU.\n",
    "    Returns\n",
    "    -------\n",
    "    - train_loader: training set iterator.\n",
    "    - valid_loader: validation set iterator.\n",
    "    \"\"\"\n",
    "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
    "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
    "\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=train_transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=valid_transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n",
    "        num_workers=num_workers, pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    # visualize some images\n",
    "    if show_sample:\n",
    "        sample_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=9, shuffle=shuffle,\n",
    "            num_workers=num_workers, pin_memory=pin_memory,\n",
    "        )\n",
    "        data_iter = iter(sample_loader)\n",
    "        images, labels = data_iter.next()\n",
    "        X = images.numpy().transpose([0, 2, 3, 1])\n",
    "        plot_images(X, labels)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "trainloader, valloader = get_train_valid_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "We use a very simple convolutional neural net, with two convolutions, each followed by a RELU. After that we have a couple of linear filters which ends up with the 10 classes.  \n",
    "The reason for this setup was to experiment with convolutions, as well to use the pre-existing setup for the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ################################################################################\n",
    "        #                                OUR CODE                                      #\n",
    "        ################################################################################\n",
    "        self.kernel_size = 3\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, self.kernel_size)\n",
    "        self.conv2 = nn.Conv2d(6, 16, self.kernel_size)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        ################################################################################\n",
    "        #                              END OF OUT CODE                                 #\n",
    "        ################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        #                                OUR CODE                                      #\n",
    "        ################################################################################\n",
    "        '''\n",
    "        Forward pass of neural net. Input-size = (3, 32, 32) Output = (10,)\n",
    "        '''\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 16 * 6 * 6)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        ################################################################################\n",
    "        #                              END OF OUR CODE                                 #\n",
    "        ################################################################################\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#                                OUR CODE                                      #\n",
    "################################################################################\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "# Loss and Optimizer (Try different learning rates)\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated. \n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.002) # Choose the optimizer you want and tune its hyperparameter\n",
    "criterion = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted\n",
    "\n",
    "################################################################################\n",
    "#                              END OF OUR CODE                                 #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch nr.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo/anaconda3/envs/deep/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 val images step 50:                 29 %\n",
      "Accuracy of the network on the 1000 val images step 100:                 31 %\n",
      "Accuracy of the network on the 1000 val images step 150:                 36 %\n",
      "Accuracy of the network on the 1000 val images step 200:                 37 %\n",
      "Accuracy of the network on the 1000 val images step 250:                 43 %\n",
      "Accuracy of the network on the 1000 val images step 300:                 41 %\n",
      "Accuracy of the network on the 1000 val images step 350:                 44 %\n",
      "Accuracy of the network on the 1000 val images step 400:                 44 %\n",
      "Accuracy of the network on the 1000 val images step 450:                 45 %\n",
      "Accuracy of the network on the 1000 val images step 500:                 46 %\n",
      "Accuracy of the network on the 1000 val images step 550:                 47 %\n",
      "Accuracy of the network on the 1000 val images step 600:                 49 %\n",
      "Accuracy of the network on the 1000 val images step 650:                 50 %\n",
      "Accuracy of the network on the 1000 val images step 700:                 47 %\n",
      "Accuracy of the network on the 1000 val images step 750:                 48 %\n",
      "\n",
      " Epoch nr.1\n",
      "Accuracy of the network on the 1000 val images step 800:                 49 %\n",
      "Accuracy of the network on the 1000 val images step 850:                 50 %\n",
      "Accuracy of the network on the 1000 val images step 900:                 52 %\n",
      "Accuracy of the network on the 1000 val images step 950:                 52 %\n",
      "Accuracy of the network on the 1000 val images step 1000:                 51 %\n",
      "Accuracy of the network on the 1000 val images step 1050:                 54 %\n",
      "Accuracy of the network on the 1000 val images step 1100:                 55 %\n",
      "Accuracy of the network on the 1000 val images step 1150:                 55 %\n",
      "Accuracy of the network on the 1000 val images step 1200:                 55 %\n",
      "Accuracy of the network on the 1000 val images step 1250:                 52 %\n",
      "Accuracy of the network on the 1000 val images step 1300:                 54 %\n",
      "Accuracy of the network on the 1000 val images step 1350:                 54 %\n",
      "Accuracy of the network on the 1000 val images step 1400:                 56 %\n",
      "Accuracy of the network on the 1000 val images step 1450:                 56 %\n",
      "Accuracy of the network on the 1000 val images step 1500:                 59 %\n",
      "\n",
      " Epoch nr.2\n",
      "Accuracy of the network on the 1000 val images step 1550:                 59 %\n",
      "Accuracy of the network on the 1000 val images step 1600:                 58 %\n",
      "Accuracy of the network on the 1000 val images step 1650:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 1700:                 59 %\n",
      "Accuracy of the network on the 1000 val images step 1750:                 59 %\n",
      "Accuracy of the network on the 1000 val images step 1800:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 1850:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 1900:                 59 %\n",
      "Accuracy of the network on the 1000 val images step 1950:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2000:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2050:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2100:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2150:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2200:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2250:                 60 %\n",
      "\n",
      " Epoch nr.3\n",
      "Accuracy of the network on the 1000 val images step 2300:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2350:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2400:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2450:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2500:                 59 %\n",
      "Accuracy of the network on the 1000 val images step 2550:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2600:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2650:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2700:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2750:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2800:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2850:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 2900:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 2950:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3000:                 59 %\n",
      "Accuracy of the network on the 1000 val images step 3050:                 60 %\n",
      "\n",
      " Epoch nr.4\n",
      "Accuracy of the network on the 1000 val images step 3100:                 62 %\n",
      "Accuracy of the network on the 1000 val images step 3150:                 62 %\n",
      "Accuracy of the network on the 1000 val images step 3200:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3250:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3300:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3350:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3400:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 3450:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3500:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 3550:                 62 %\n",
      "Accuracy of the network on the 1000 val images step 3600:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3650:                 62 %\n",
      "Accuracy of the network on the 1000 val images step 3700:                 60 %\n",
      "Accuracy of the network on the 1000 val images step 3750:                 61 %\n",
      "Accuracy of the network on the 1000 val images step 3800:                 62 %\n"
     ]
    }
   ],
   "source": [
    "#traindataset = utils.TensorDataset(X_train, y_train)\n",
    "#trainloader = utils.DataLoader(traindataset, batch_size=64, shuffle=True)\n",
    "\n",
    "epochs = 5\n",
    "steps = 0\n",
    "print_every = 50\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    print('\\n Epoch nr.{}'.format(e))\n",
    "   \n",
    "    # Reduce learning rate on third epoch as we did not see performance gain after epoch 3\n",
    "    if e == 2:\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr = 0.0005) # Choose the optimizer you want and tune its hyperparameter\n",
    "    \n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        ################################################################################\n",
    "        #                                OUR CODE                                      #\n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(images)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ################################################################################\n",
    "        #                              END OF OUR CODE                                 #\n",
    "        ################################################################################\n",
    "    \n",
    "        if steps % print_every == 0:\n",
    "            stop = time.time()\n",
    "            # Test accuracy\n",
    "            net.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data in valloader:\n",
    "                    images, labels = data\n",
    "                    outputs = net(images)\n",
    "                    _, predicted = torch.max(F.softmax(outputs).data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "                print('Accuracy of the network on the %d val images step %d: \\\n",
    "                %d %%' % (total, steps, 100 * correct / total))\n",
    "\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model should be saved to be tested on the test dataset or to be used in a real-life application. To save a model in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./model.ckpt\")\n",
    "net.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo/anaconda3/envs/deep/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def run(X):\n",
    "    '''\n",
    "    Test function for run.py file\n",
    "    '''\n",
    "    net = Net()\n",
    "    checkpoint = torch.load(\"./model.ckpt\")\n",
    "    net.load_state_dict(checkpoint)\n",
    "    pred = np.empty(0)\n",
    "    l = np.empty(0)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(F.softmax(outputs).data, 1)\n",
    "            pred = np.append(pred,predicted.numpy())\n",
    "            l = np.append(l,labels.numpy())\n",
    "\n",
    "    return pred.astype(int), l.astype(int)\n",
    "pred, labels = run(valloader)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.619"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test run function\n",
    "np.mean(pred == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
