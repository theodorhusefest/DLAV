{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built in Part 1 is dumb, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial C}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial C}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "In PyTorch, the gradients are calculated automatically. It does this by keeping track of operations performed on tensors. For PyTorch to track operations and calculate the gradients, you need to set `require_grad` = `True` when creating a tensor.\n",
    "\n",
    "The gradients of a tensor (e.g.,`z`) are computed with respect to the variable that created it using `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12.],\n",
       "        [12., 12.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 3*(x+1)**2\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a `MulConstant` op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x120bfb510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.mean()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can check out how `z` was created, with a `Mean` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MeanBackward0 at 0x120c0f610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't performed a backward pass, `x` doesn't have a gradient yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the gradients of `z` with respect to `x` with `z.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the layers, 200, 50, 10 units each\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' This function for predicts classes by calculating the softmax '''\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0133, -0.0168, -0.0356,  ...,  0.0050, -0.0122, -0.0008],\n",
      "        [-0.0279,  0.0185,  0.0092,  ...,  0.0328,  0.0175, -0.0206],\n",
      "        [-0.0318,  0.0319, -0.0265,  ...,  0.0228, -0.0102, -0.0212],\n",
      "        ...,\n",
      "        [-0.0189,  0.0308,  0.0189,  ..., -0.0253,  0.0329,  0.0331],\n",
      "        [-0.0287,  0.0175, -0.0344,  ...,  0.0190, -0.0123,  0.0225],\n",
      "        [ 0.0025, -0.0166,  0.0195,  ..., -0.0158,  0.0173,  0.0198]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 1.9312e-03,  1.9312e-03,  1.9312e-03,  ...,  1.9312e-03,\n",
      "          1.9312e-03,  1.9312e-03],\n",
      "        [-3.5391e-04, -3.5391e-04, -3.5391e-04,  ..., -3.5391e-04,\n",
      "         -3.5391e-04, -3.5391e-04],\n",
      "        [-4.7961e-04, -4.7961e-04, -4.7961e-04,  ..., -4.7961e-04,\n",
      "         -4.7961e-04, -4.7961e-04],\n",
      "        ...,\n",
      "        [-3.2569e-03, -3.2569e-03, -3.2569e-03,  ..., -3.2569e-03,\n",
      "         -3.2569e-03, -3.2569e-03],\n",
      "        [ 7.1760e-05,  7.1760e-05,  7.1760e-05,  ...,  7.1760e-05,\n",
      "          7.1760e-05,  7.1760e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', net.fc1.weight)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "inputs, targets = dataiter.next()\n",
    "\n",
    "inputs.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients from all Tensors\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = net.forward(inputs)\n",
    "loss = criterion(output, targets)\n",
    "loss.backward()\n",
    "print('Gradient -', net.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will output the updated weights that are updated using SGD with learning rate (0.01). Since both the learning rate and gradients are small, the changes of the weights might not be drastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0134, -0.0168, -0.0356,  ...,  0.0050, -0.0122, -0.0008],\n",
      "        [-0.0279,  0.0185,  0.0092,  ...,  0.0328,  0.0175, -0.0206],\n",
      "        [-0.0318,  0.0319, -0.0265,  ...,  0.0228, -0.0102, -0.0211],\n",
      "        ...,\n",
      "        [-0.0189,  0.0308,  0.0190,  ..., -0.0253,  0.0329,  0.0332],\n",
      "        [-0.0287,  0.0175, -0.0344,  ...,  0.0190, -0.0123,  0.0225],\n",
      "        [ 0.0025, -0.0166,  0.0195,  ..., -0.0158,  0.0173,  0.0198]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', net.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo/anaconda3/envs/deep/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([16, 784])\n",
      "Epoch: 1/1 Loss: 2.1144 Test accuracy: 0.4844\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([16, 784])\n",
      "Epoch: 1/1 Loss: 1.6446 Test accuracy: 0.6919\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([16, 784])\n",
      "Epoch: 1/1 Loss: 1.1467 Test accuracy: 0.6920\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-75cbac1cba03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_getencoder\u001b[0;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# get encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "for e in range(epochs):\n",
    "    for inputs, targets in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        inputs.resize_(inputs.size()[0], 784)\n",
    "        print(inputs.size())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Test accuracy\n",
    "            accuracy = 0\n",
    "            for ii, (inputs, targets) in enumerate(testloader):\n",
    "                \n",
    "                inputs = inputs.resize_(inputs.size()[0], 784)\n",
    "                \n",
    "                predicted = net.predict(inputs).data\n",
    "                equality = (targets == predicted.max(1)[1])\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every),\n",
    "                  \"Test accuracy: {:.4f}\".format(accuracy/(ii+1)))\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "inputs, targets = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo/anaconda3/envs/deep/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAroAAAGHCAYAAABf8fH3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debgkVX3/8fcXZBm2gVGURWRY1AFRcEYWNWEVUHFXYhR/ERX3PRolmEVjIhijaNwVgSC4oWJEFDEqYsBBGcQEHFCWAQQdZBtAhnW+vz/q3ExzuX236b6nu+779Tz9VFfVqarvrW4unzn3VFVkJpIkSVLbrFW7AEmSJKkfDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJI0hIvaNiIyIZT3e7+Flv2f3cr/DICJOLD/7e4fluON9D8bbb1meETF/GiWrRwy6kqRW6Qgfna97I+KmiLg8Ir4VEUdFxHa1ax2thOD3RsRua7CP947x82dE3BERSyPiUxHx2F7WrcmLiN3KZ3R47VpmA4OuJKmt7gWWl9fNwAbADsBzgX8BroiIUyNi8y7b3wlcBlzR47pWlP1eM8a6w4F/BKYddDusYvXPvxxYH1gAvB74VUS8qAfHmA2m+z24rLzuHbV8N5rP+PA1rkwTekjtAiRJ6pPzMnPfzgURsSmwF03IOBR4EfDkiNgzM6/rbJuZP6cJhj2VmacBp/V6v2O4NjPnj8xExLrAQcCngUcCJ0XEeZl5/QzUMrSm+z3IzJ5/dzR19uhKkmaNzLw1M8/MzL8EDgHuArYGvlG3sv7LzHsy8zvAYWXRHODlFUuS+s6gK0malTLzTOCdZXbPiHh25/rJXIwWES+PiPMj4k8RcXNE/DginlXWLSvb7ztqmwddjDayDNinLDph1PjarjVMVWaeA4z0Xi/qqOEBP29EPCMivhcRN0TEqoh426if4xER8eGIuDQi7oyIFRHx84h4R0SsN1EdEbF+RLyvbL+yHOfLEfGYcbbZMyKOjojFEXFdRNxTtjtzskMxpnrc6V6UONbFaOUzPqHM7jPGOOp9I2Lv8v7uiHjoOPvfvnwu6Zjr7gy6kqTZ7PM041cBXjqVDSPi88CJwB4041+DJqieHhFvnWIdK0sdI+M5b+OB42v/OMX9TWQk6G4y1sqIeAfwXeBgYB2a8b6d6/cAfg38NfBY4D5gXWB34N+A8yPi4eMcfz3gx8A/ANsB9wCbA38J/DIi9h6jpo2AxcCRwJ6l/V1lejBwakR8doKfe8rH7bHlNJ8tPHAM+cjrnvIPkd/QnM/xvpOvoPnOnZuZl/Wt4iFn0JUkzVqZeQ/wozL755PdLiJeARxRZo8G5mXmZsAWwBeAD9EEqMnW8dXM3AI4ryx6a2Zu0fHafbL7mqRHlemtY6x7BPBB4FPAluXn2gj4OkBEbAZ8C5gH/C+wR2ZuUtocCtwC7AqcMs7xXw88gWboxEaZORd4InAhzUWDXyvH6bSKJny/hGa4yfrluJsBbwbuAF4TEYf2+Lg9Uz7jkX8EnTfqM94iM0c+/y+U6SvG2k9ErMXqYSfH96veNjDoSpJmu/8t060jYp2JGkdE0PQIAnw+M4/KzBUAmXlDZh4B/IAmOA2ciDiEJpADnD9Gk/WBr2XmGzNzOUBm3pWZvyvr3wRsSROSD8rMX5Q292fm12l6RwGeFhH7dyljLvCazDwpM+8t219E0zN7E03YfmPnBpl5Z2YekplfyczrM3NVWX5rZn4CeENp+ga6m/JxKzmRpsf3iRGx6xjrnwZsQxPuvzaDdQ0dg64kaba7peP9vEm0XwjML+//tUubD65JQf0QEVtFxKuAk8qi2zrej/ahcXY1Mhb2uMz8w+iVmXkW8LMy+xdd9nE18KUxtr0RGBl+MNXbn51epntFxNozeNyey8wbWP3zvHKMJiM9vadm5h0zU9VwMuhKkrRaTqLNE8v0D5l5eZc2i3nw/VNn2radFzrRjMs9jibMrwAOzcyxxv6uBH411g7LLcp2KbM/HufYI8NBFnZZ/5PM7Hauf1Kmu5TjdR7/IRHxqnLx2e/LBVsjP9/IP1jWpxnO0LPjVnJcmR7WWU+5Rd7zyuwXHrSVHsD76EqSZrvOUHRL11arPaxMf9+tQWbeExE3sXqIQA2rWH0RW9IE2GuAs4HPjXP/3JtGhgWMYR6rO8mu69IGYGSYQ7dxyuNtO7JubZrPZjn838Vo3wee0tF2Jc3POFLvI8p0Q+DGXhy3ou8D19IMUXg2q2+BdxhNmL8sM8+tVNvQMOhKkma7x5fp70bGbU4g+llMDz3ggRFTcP8k2014C7Fp6nZ+/54m5N4IvAM4s/yJv9moGa5w3wT7mM5xq8jMVRFxPM1T1F7B6qA7MmzhhDE31AM4dEGSNGuVPwkfUGZ/OsnNRnpJt5xgv13vgTrEbmZ17+m247R7ZJl2uy3aVuNsO3Je7+eBPewjd1N4c7mY7IYHbvZ/vbnjmc5xazqe5nw/PSK2jIjH09z7+H66j69WB4OuJGk2ezUwcr/X8W6H1emXZbpFROzQpc2eNPefnaqREDlQvYsjyu3YLi6z+43TdORuCxd2Wb9Pl+Wd6y4uxxsxEp5/ydieNs4+1+S4vTbpzzgzr6G5g8fawF+x+sK072Vm16EzWs2gK0malSLiYFbfXeBnmXnGJDe9kObqfVj9ZLXR3jXNskYeJrDpNLefCV8v08Mj4kG92hFxEPDkMtvt1lfzI+IlY2w7D3hNmT111OoVZfr4UctHxu++Z4K6p3vcXpvqZ/z5Mn0lqx/f7EVok2TQlSTNGhExNyIOjogv0zx8YA7NBT+TvqVUuWr//WX2dRHx/ojYpOx/84j4HM19We+cRomXlOkLImLuNLafCZ+guRBvDnBmRDwJmjGyEfFC4Cul3X9l5o+67GMF8PmIeFlEPKRs/wSaC7A2B26geWBFpx+U6UciYp9yP2MiYnfgh6y+SHA80zlur418xjtHxJ6TaP9tmroew+oaJ/uPslnPoCtJaqunRMQfOl5/onnIwZk0DzUImh7HhePcgaCb41l9MdDfATdHxM00V+ofQfNo3JGr/u+ewn6/SPNY2j8DboyI6yJiWUT89xTr65vMvIXm9la30Dxl7BcRcRvNwwu+TnPHgv9hde/jWD5N86COLwJ3RMQKmluaPYnmHwiHluN0+juac7oNzZ0j7oyIO4Cf0/TyPqintkfH7anM/C1wDs0NARZHxE3lM14WEXuN0f5eHjge94uTvGhSGHQlSe21Ds0FSo+g6e27G7iSpofsPcAOmfni8rCAKcnGK2n+nPyLsu+gCWCHlCd1bVKaj/WY3W77vRQ4kCaMr6C5Pdm2rB6fOhAy8+fAzsCxwG9ozvV9wAXA3wB7jnGxWKe7acb4/hPNMJB1aS5c+wrNPzzOGeOYVwJ7ACfT9GquTXNuTwF2Lw+qmMiUj9snL6DpOb6K5tHJ25bX+l3af7PjvY/8nYLoft9kSZI0HeUitctpemc37vPFTWq5iHgP8M/A+Zn5oF5fdWePriRJvTdyMdo5hlytiXJ/4CPK7Odq1jKMDLqSJE1DRJwQES+KiId2LNsuIj7F6iv4P1ynOrVBueDuH4D5NOO/v1y1oCHkk9EkSZqeA4HDAcqFbquAjTvW/0tmnlmhLg25clHaV2gu7BsZ631UZq6sV9VwMuhKkjQ9fwM8F3gizQVvGwDXAz8DPjXOrbWkiaxPc3HavcClwEcy04vQpsGL0SRJktRKjtGVJElSKxl0JUmS1EoGXUmSJLXStC9GO3CtQx3cK2lo/WDVqVG7BklSf9mjK0mSpFby9mKSNAtExFU09+NcVrkUSZqq+cBtmbndVDc06ErS7LDJnDlz5u20007zahciSVOxdOlSVq6c3rMyDLqSNDss22mnneYtWbKkdh2SNCWLFi3iwgsvXDadbR2jK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWukhtQuQJM2Mi69bwfwjz3jQ8mXHHFKhGknqP3t0JUmS1EoGXUmSJLWSQVeSKovGYRHxw4i4KSLuioirIuIzEbFd7fokaVgZdCWpoohYBzgNOBnYH9gEuBOYD7wW+J+I2L9agZI0xAy6klTXB4HnAvcBbwfmZuY8YBvgVGAj4JsRsWW9EiVpOBl0JamSiHg48MYy+5HM/Ghm3gmQmb8DXgIsBeYCf1enSkkaXgZdSapnf2Dd8v7Y0Ssz837g38vsS8owB0nSJBl0Jamebct0RWb+oUubS8t0M2Bh/0uSpPYw6EpSPVmm4/0u7nywz+P6WIsktY5PRpOkeq4u040jYpvMvHaMNjt3vN9qoh1GxJIuqxZMtThJGnb26EpSPT8G7inv3z16ZUSsC7ytY9HGM1GUJLWFPbqSVElm3hARnwHeArwhIlYAnwaWA7sAHwK2A+4F1gFWTWKfi8ZaXnp6HeMraVaxR1eS6noXcDoQwFHAtTS9vBcCBwCfBK4sbW+tUaAkDSt7dCWposy8OyKeC7wIOIzmgrO1ae628HmaEHxbaf7bKkVK0pAy6EpSZZmZNE9BO3X0uojYA5hTZhfPZF2SNOwcuiBJg+0VZXp2Zl5ftRJJGjIGXUkaUBHxZOCIMnt0zVokaRgZdCWpoojYLyLeHhHbR8TaZdlmEfFm4Ps0Q8w+l5lnVS1UkoaQY3Qlqa5tgY+U130RcQcwl+YuDADHAW+oVJskDTWDriTV9d/Ax4C9gUfRPBTid8C5ND25P65YmyQNNYOuJFWUmZfzwKefSZJ6xKArSbPELlvPZckxh9QuQ5JmjBejSZIkqZUMupIkSWolg64kSZJayaArSZKkVjLoSpIkqZUMupIkSWolg64kSZJayaArSZVFxFoR8YqI+K+I+GNE3BsRt0bE+RHxnojYuHaNkjSMfGCEJFUUERsApwP7dyy+DdgE2KO8Xh0R+2fmlRVKlKShZY+uJNX19zQhN4GjgE0zcy6wPvAS4FZgW+C4ahVK0pCyR1eS6nppmZ6QmUePLMzMe4CvRMT6wAnAfhGxWWbeUqNISRpG9uhKUl2PKNNfdlm/pOP9Bn2uRZJaxaArSXUtK9Mndlm/qEyXA9f3vRpJahGDriTV9fkyfUVEHBkRcwEiYt2IeDFwLM343XdmZtYqUpKGkUFXkur6KPBJIICjgVsj4lZgJfAV4FLgOZl5cr0SJWk4eTGaJFWUmfdHxNuAK4EP0vxentvRZGNg88nuLyKWdFm1YNpFStKQskdXkiqKiC2Ac4EPA6cAuwIbAY8G/hbYHjg+Io7uuhNJ0pjs0ZWkuk6ieSjEFzLziI7llwPHRMR1pc27IuKUzLx4vJ1l5qKxlpee3oU9qlmShoI9upJUSUTsDBxYZo8dq01mfhG4ieb39bNmqDRJagWDriTVs1PH+6vGaTfy6N/5/StFktrHoCtJ9azqeP+ocdptW6a397EWSWodg64k1XNRx/tXj9UgIp4NPLzMnt/3iiSpRQy6klRJZl4FnFVm3xYRR0fEwwEiYqOIOBw4saxfBnx7pmuUpGFm0JWkug4HltL8Pj4SWB4Rt9EMUzgBmEfz+N8XZOY9tYqUpGFk0JWkijLz98Ai4G3AOcDNwAbAbcCFwPuBx2fmL6sVKUlDyvvoSlJlmbkS+Fh5SZJ6xB5dSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa3k7cUkaZa4+LoVzD/yjNplSBoCy445pHYJPWGPriRJklrJoCtJkqRWMuhKUiURkVN47VO7XkkaNjMyRvfyY/eadNun7vXrSbc9d/HO0yln1rrixZ+pXcJQ2eGrr5t02x3fvriPlajFlk+wfhNgDnAPcHH/y5GkdvFiNEmqJDO3GG99RFwE7Ap8JzNvmpmqJKk9HLogSQMoInajCbkA/1GzFkkaVgZdSRpMLy/TPwLfrVmIJA0rg64kDZiIeAjw0jJ7SmbeV7MeSRpWBl1JGjzPAB5e3jtsQZKmyYvRJGnwHF6m/5OZF01lw4hY0mXVgjWqSJKGkD26kjRAImIe8Kwye2LFUiRp6NmjK0mD5SXAusB9wClT3TgzF421vPT0Llyz0iRpuNijK0mDZeRuC9/LzBuqViJJQ86gK0kDIiJ2AnYvs16EJklraEaGLvTt0bPbntOf/UpM8Xv74sk3/fM3vnbSbTc47fzJ71htcHiZ3gycXrEOSWoFe3QlaQBExFrAy8rslzPznpr1SFIbGHQlaTAcCGxV3jtsQZJ6wKArSYNh5CK0X2fmL6pWIkktYdCVpMoiYhPgeWXW3lxJ6hGDriTV9xfAHGAVcHLlWiSpNQy6klRZZh6XmZGZa2fm9bXrkaS28MlokjRL7LL1XJYcc0jtMiRpxtijK0mSpFYy6EqSJKmVDLqSJElqpRkZo/tXV+896bYn+VjfgbDDV1/Xl/327XHQQ2S7dy2ddNvlp/WxEEmSWs4eXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUkaEBGxfUQcGxFLI+KOiFhR3h8fEfvUrk+Sho1PRpOkARARrwQ+Acwpi/4ErAMsKK9VwE/qVCdJw8keXUmqLCL+EjiOJuR+AtghMzfKzA2ALYD/B5xXsURJGkr26EpSRRHxcOBTQABHZebRneszczlwco3aJGnY2aMrSXW9HtgMuAz4YOVaJKlVDLqSVNdhZXpSZq6qWokktcyMDF1Y/uTbJt32YHbrYyWarB1Z3Jf9Hvz2up/vnc/fc9Jtf/rJz/alhqk85tr/HtotIh4KPLrM/ndE7A+8G9gDWA9YBnwb+LfMvLFKkZI0xByjK0n1PLrj/UHAUTRjdW8vy3Yqr5dFxIGZuXSiHUbEki6rFqxJoZI0jBy6IEn1bNrx/ijgEmDPzNwE2Ah4JnADsDXwjYiwc0KSpsBfmpJUT2dnw/3A8zPzcoAyXvd75f6636Hp2X0+cOp4O8zMRWMtLz29C3tRtCQNC3t0JameOzrenzEScjtl5hnAb8rs02akKklqCYOuJNVzfcf7y8ZpN7Jumz7WIkmtY9CVpHquBFaW9zmJ9pNpI0kqDLqSVEkZh3t2mR3vrgiPLdOr+1qQJLWMQVeS6vpimR4SETuOXhkRhwCPKbPfnbGqJKkFDLqSVNdXgSU0d8E5LSJ2B4iItSLi6cAXSrufA2fUKVGShpO3F5OkijJzVUQ8D/gJsAvw84i4HVgb2KA0uwx4UWY6RleSpsCgq1llg9POn3zjT/avjsm6/Ni9Jt12x7f357HN6r/M/F1E7Aq8E3ghsD3NhWe/BL4O/Htm3jHOLiRJYzDoStIAKEH2veUlSeoBx+hKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKUkURcXhE5AQvn4omSdPgk9EkaTDcC9zcZd2fZrIQSWoLg64kDYbzMnPf2kVIUps4dEGSJEmtZNCVJElSKxl0JUmS1EoGXUkaDI+LiEsiYmVE3B4RF0fEsRGxXe3CJGlYGXQlaTA8DNgJuBNYH3gc8Dbgkoh4ac3CJGlYedcFSarreuAfgW8Av83MeyJiPeAA4EPAzsBJEfG7zDxnop1FxJIuqxb0qmBJGhYGXUmqKDPPAs4atexu4LsRcS5wAbAjcAzwlJmvUJKGl0FXkgZUZq6IiA8AxwN7RcTmmfnHCbZZNNby0tO7sA9lStLAcoyuJA2288s0gPkV65CkoWOPrmaVO5+/5xRaX9S3OiZrq3OydgmqLzre+4WQpCmwR1eSBtseHe+vrlaFJA0hg64kVRIRMcH6TYAjy+zPJxqfK0l6IIOuJNWzbUQsjohXRcSjRhZGxLoR8XTgXOAxwCrgb2sVKUnDyjG6klTXnuVFRNwF/AnYBFinrL8TeF1m/qhOeZI0vAy6klTPcuAtwJ8BuwKbA3Npwu5vgR8Cn85Mx+ZK0jQYdCWpksxcCXy8vCRJPeYYXUmSJLWSQVeSJEmtZNCVJElSKzlGV7PK9XuPe9vSgbPBaedP3EiSJI3JHl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkgZMRGwUEddGRJbX4bVrkqRhZNCVpMHzz8AjaxchScPOoCtJAyQiFgJvAryJsiStIYOuJA2IiFgL+GyZfX3NWiSpDQy6kjQ43gw8Cfh0Zv6ydjGSNOx8BLBmlafu9evaJUhjioitgfcDy4G/q1yOJLWCQVeSBsPHgY2BN2TmitrFSFIbGHQlqbKIeDbwfODszDx5Dfe1pMuqBWuyX0kaRo7RlaSKImJD4BPAvcAbK5cjSa1ij64k1fVPwKOAf83MNR5EnpmLxlpeenoXrun+JWmY2KMrSZVExG7AW4FraQKvJKmH7NGVpHo+BqwNvAeIiNioS7v1yrpVmXnnjFUnSUPOHl1JqmfbMj0JuH2M14jPlHnvjydJU2DQlSRJUisZdCWpksycn5nR7dXR9BVl2fxatUrSMDLoSpIkqZW8GE2zyknbnlO7BEmSNEPs0ZUkSVIr2aMrSQNq1DhdSdIU2aMrSZKkVjLoSpIkqZUMupIkSWolg64kSZJayaArSZKkVjLoSpIkqZUMupI0S1x83QrmH3lG7TIkacYYdCVJktRKBl1JkiS1kk9Gk6SKIuJJwHOB3YEdgc2B9YEbgQuAEzLzW/UqlKThZdCVpLqOAF7bMX8HsArYCngO8JyI+Abwksy8t0J9kjS0HLogSXX9DHg7sAjYODM3zsw5wKOAD5U2LwSOrFSfJA0te3QlqaLM/I8uy68F3hURWwIvAw4H3j+DpUnS0LNHV5IG2y/KdKuqVUjSEDLoStJge0qZXlW1CkkaQg5dkKQBExEbAdvTXKT24rL4E/UqkqThZNCVpAEQEY8Erh1j1V3ABzLzU5Pcz5IuqxZMtzZJGlYGXUkaDPcDy8v7zYB1gfuAo7E3V5KmxaArSQMgM38PbAEQEWvRPDzi3cD7gFdFxDMz85JJ7GfRWMtLT+/C3lUsSYPPoCvNsB2++rpJt92RxX2sRIMqM1cBv6EJuLcCfw2cHBGLyjpJ0iR41wVJGmwfL9PdgCfWLESSho1BV5IG23Ud73eoVoUkDSGDriQNtu063t9RrQpJGkIGXUmqJCLWjoiYoNnflOl9wM/6XJIktYpBV5Lq2Qa4ICJeWe6jCzR3XYiI3SLiFOCIsvjjmXlLlSolaUh51wVJqmsh8AWAiLiLZnjCxsB6HW1OBN4145VJ0pAz6EpSPdfTPOL3AGAPYEvgoTRPQ7uCZqjCCZl5brUKJWmIGXQlqZLMvAf4WnlJknrMMbqSNEvssvVclh1zSO0yJGnGGHQlSZLUSg5d0NC78/l7TqH1RX2rY7K2OidrlyBJ0qxgj64kSZJayaArSZKkVjLoSpIkqZUMupIkSWolg64kSZJayaArSRVFxKMi4m0RcXpEXBMRd0fE7RHxq4g4JiK2rF2jJA0rby8mSZVExDbAMiA6Ft8GbAg8obxeExEvzMwfz3yFkjTc7NGVpHrWLtMzgEOBeZk5F9gAeCZwFbAZ8K2I2KJOiZI0vAy6klTPLcATM/NZmfn1zLwFIDPvyczv0YTdu4BNgNdWrFOShpJBV5IqycwVmfmrcdZfCiwus4tmpipJag/H6GroXb93TNxogGxw2vm1S9BwualM1x63lSTpQezRlaQBFREPAZ5aZi+uWYskDSODriQNrjcCWwCrgJMq1yJJQ8ehC5I0gCLiCcAHyuwnMvOSSW63pMuqBT0pTJKGiD26kjRgykMivkVzm7ElwLvrViRJw8keXUkaIBExDzgL2A74LXBIZt412e0zc8y7M5Se3oU9KVKShoQ9upI0ICJiLvB9YBfgGuBpmbm8blWSNLwMupI0ACJiQ+C7wJOAP9CE3GvqViVJw82gK0mVRcQc4HTgKTT3zX1aZv62blWSNPwMupJUUUSsC3wT2A+4FThosndYkCSNz6ArSZVExNrAl4CnA7cDz8jMC+tWJUnt4V0XpBl2+bF7Tbrtjm9f3MdKNACeCrywvF8H+FZE10daX5uZu89IVZLUEgZdSaqn869q65dXN5O+xZgkqWHQlaRKMvNsoGsXriRpzThGV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZIPjJBm2FbnZO0SJEmaFQy6klRRRGwM7AfsDjypTB9aVu+UmZfWqk2Shp1BV5LqOgA4rXYRktRGBl1Jqu8G4ALgF8B1wOfqliNJ7WDQlaS6Ts/Mb43MRMT8eqVIUrt41wVJqigz769dgyS1lUFXkiRJrWTQlSRJUis5RleSWiQilnRZtWBGC5GkAWCPriRJklrJHl1JapHMXDTW8tLTu3CGy5Gkqgy6GnpXvPgztUuYkg1OO792CZIkzQoOXZAkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa3kxWiSVFlEPKxjdrOO95uOWndzZq6aobIkaegZdCWpvj92Wf6zUfPbAcv6W4oktYdDFyRJktRK9uhKUmWZGbVrkKQ2skdXkiRJrWSPrtQDO3z1dZNuuyOL+1iJJEkaYY+uJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVDLqSNAAiYouI+FhEXBERd0XE8og4PSIOqF2bJA0rg64kVRYRTwAuBt4CbA/cDTwMeBbwg4g4smJ5kjS0DLqSVFFEzAG+DTwU+CWwS2bOBTYDPgwEcHREHFSvSkkaTgZdSarrtcC2wB3AszPzEoDMvC0z3wl8q7Q7ulJ9kjS0DLqSVNdhZfqlzLxujPUfKtOFEbFghmqSpFYw6EpSJRGxMbCozH6/S7PFwIryfv++FyVJLWLQlaR6dqIZgwtwyVgNMnMVcFmZ3XkmipKktnhI7QIkaRbbsuP99eO0GyEjlgEAAAopSURBVFm35ThtAIiIJV1WOexB0qxjj64k1bNhx/uV47S7s0w36mMtktQ69uhKUj0xcZOpycxFYy0vPb0Le308SRpk9uhKUj13dLyfM067DcZoL0magEFXkurpHJe71TjtRtb9vo+1SFLrGHQlqZ5LgSzvHzdWg4hYC3hsmf31TBQlSW1h0JWkSjLzduCCMntgl2Z7AnPL+x/2vShJahEvRtPQ+6ur955025O2PacvNez49sV92a9mhS8BuwOHRcQ/Zebo4QnvLNMlmXkZkqRJs0dXkur6LHA1sDHwnYjYGZqnpkXEvwIvKO2OqlSfJA0te3QlqaLMXBkRz6UZlrAQuCQibqO5Z+5aNGN4j8rMsyqWKUlDyR5dSaosM38F7AL8O3AlsB5wE3AGcGBmHlOxPEkaWvboStIAyMw/AG8tL0lSD9ijK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVvL2Yht7yJ9826bYHs1sfK5EkSYPEHl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJreTtxSRpdpi/dOlSFi1aVLsOSZqSpUuXAsyfzrYGXUmaHTZauXLl/RdeeOGvahcyQBaU6aVVqxgsnpMH85w82Eyfk/nA5G+a38GgK0mzw8UAmWmXbhERS8Bz0slz8mCekwcbpnPiGF1JkiS10rR7dH+w6tToZSGSJElSL9mjK0mSpFYy6EqSJKmVDLqSJElqpcjM2jVIkiRJPWePriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kjTAIuKREXF8RFwfEXdHxLKI+GhEbDbF/cwr2y0r+7m+7PeR/T52r61pXRGxYUQcFhFfiohLI+JPEXF7RFwQEe+IiHW7bJfjvBb39qecml58VhFx9gQ/4/pdtts5Ir4WETdExF0RcVlEvC8i5vTuJ5y6HnxP9p3gfIy8thm13UB+TyLiRRHx8Yj4aUTcVuo5eZr7mvK5rfU98YERkjSgImIH4Dzg4cB/ApcCewD7AZcBT83Mmyaxn4eW/TwG+BHwC2AB8FzgBuDJmXllP47da72oKyKeDnwPuBn4MXA5MA94NrBF2f8BmXnXqO0SuBo4cYzd/i4zj5v2D7YGevg9ORvYB3hflyb/nJn3jdpmT5rv1DrA14Frgf2BJwHn0pzHu6f+U62ZHn1P5gOHd1n9eOAFwCWZucuo7Qb1e3IRsCtwB/A7mt8Bp2Tmy6a4nymf26rfk8z05cuXL18D+AK+DyTw5lHLP1KWf2aS+/lsaf+RUcvfUpaf2a9jD+I5AXYDDgPWHbV8Y2BJ2c87xtgugbNrfy/6+D05u4kFkz7u2sCvyzGe07F8LZowk8CRw3xOxtn/l8t+3jJE35P9gEcDAexb6jy53+e29vfEHl1JGkARsT1wBbAM2CEzV3Ws2xj4Pc3/sB6emX8aZz8bAn8EVgFbZubtHevWKseYX45xZS+P3WszUVdEvBQ4BfhOZj571LoEfpKZ+07rB+iDXp6TkR7dzIxJHnt/4IfAOZm5T5e6rga2yxkMG/3+npS/kFxH89/U1pl5y6j1A/c9GS0i9qX5a8aUenSnc25rf08coytJg2n/Mj2r838mACWsngtsAOw1wX6eDMwBzu0MuWU/q4Czyux+fTh2r81EXfeW6X1d1m8aEa+MiKMi4o0RMdPnYLSen5OIeHFEHBkRfx0Rz4iI9SY49pmjV5R/NP0G2BbYfrLH7pF+f08OB9YDTh0dcjsM2vekV6Zzbqt+Twy6kjSYHlumv+my/rdl+pg+7KdXx+61majrlWX6oP8pF7sCXwD+BfgE8LOIuCgiHr8Gx1wT/TgnXwGOBj4MfBe4JiJeNEPH7oV+13VEmX52nDaD9j3plaH7fWLQlaTBNLdMV3RZP7J80z7sp1fH7rW+1hURbwKeDlwEHD9Gk48ATwU2pxnPuzvNGMNdgR9FxNbTOe4a6uU5+U+aC/IeSfNXgAU0gXdT4KsR8Yw+HruX+lZXROxDc14uyczzujQbxO9Jrwzd7xODriQNp5FxlGs6pm06++nVsXtt2nVFxAuAjwJ/AF6YmfeObpOZ78jM8zLzxsy8IzMvyMxDgW8ADwPeuQa198ukz0lmHpuZ38nM6zLzrsy8LDOPAt5Bkxc+0K9jz7A1qes1Zdq1N3dIvye9MnC/Twy6kjSYRno55nZZv8modr3cT6+O3Wt9qSsinkfz5/obgH1z1K3WJuEzZbr3FLfrhZn4rI6jGbO8W7ngaCaPPR39+p7MA14IrAS+OI26an5PemXofp8YdCVpMF1Wpt3GrT26TLuNe1uT/fTq2L3W87oi4lDgVGA5zR0HLptgk7H8sUw3nMa2a6rvn1U29xMeuZCx82ecNd+T4uU0F6F9LTNvnUZdNb8nvTJ0v08MupI0mH5cpgeV24D9n9Kr9lSanqWJnrS0uLR76qjeuJHbix006ni9PHav9bSuciuxLwPX04Tc306wSTcjV5hPtSe4F/r+WUXEY4HNaMLujR2rflSmTx9jm+1pgs3VzPx56dc5eXWZfm6addX8nvTKdM5t1e+JQVeSBlBmXkFz66/5wBtHrX4fTa/QSZ33AY2IBRGxYNR+7qD5M+uGwHtH7edNZf/f7/xz/XSOPRN6dU7K8pfTnJdrgL0nGq4QEQvLPYlHL38CzZX1ANN6nOqa6NU5iYjtx7pIKiIeBpxQZr+SD3wy2k+ApcDeEfGcjm3WAj5YZj8zk/fQhd5+TzrW/zmwE3DxOBehDez3ZKoiYp1yTnboXD7N3w1Vvyc+MEKSBtQYj9pcCuxJc8/b3wBPyY5HbZYb1TP6hv9jPAL45zT/0x55BPBTyv/Apn3smdKLcxIR+wH/RdPZczzN40hHuzUzP9qxzYk0j3z9UWl/N83V90+nefLT54HXznSoK7X14pwcTjMW9yc0N/C/GXgU8EyasZUXAAeO/pP9GI92vQY4gMF7BPC0/tvpWP9F4GU0T0L7+DjHPZHB/Z48D3hemd0COJimF/WnZdmNmfnO0nY+cBVwdWbOH7WfKf9uqPo9meqj1Hz58uXL18y9gG1oetR+D9xD8ye+jwHzxmibdHmEKzCvbHd12c/vaULeI3tx7GE6JzQ3/M8JXstGbfM84JvA5cBtHefwdDoeazrE5+TxwInA/wI30Tw442aaEPRmRj0uedS2O9OMc76RJtj9hqZ3b84wn5OOdZvR/Dn+TmDTCY45sN8Tmr/oTOo7T9Nj+6D/DqZzbmt/T+zRlSRJUis5RleSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSQVeSJEmt9P8B1g4o06pnhjMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = inputs[0]\n",
    "ps = net.predict(img.resize_(1, 784))\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
