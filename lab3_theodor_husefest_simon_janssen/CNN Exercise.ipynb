{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks\n",
    "\n",
    "We'll check out how to build a **convolutional network** to classify CIFAR10 images. By using weight sharing - multiple units with the same weights - convolutional layers are able to learn repeated patterns in your data. For example, a unit could learn the pattern for an eye, or a face, or lower level features like edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import importlib\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from shutil import copy\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from socket import gethostname\n",
    "import time\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary  # install with 'pip install torchsummary'\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from utils import plot_images, get_train_valid_loader, plot_weights, plot_gradient_flow, read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "filepath_config = 'configs/config.json'\n",
    "CONFIG = read_json(filepath_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "models = importlib.import_module('models')\n",
    "ConvNet = getattr(models, CONFIG['parameters']['model'])\n",
    "print('%s loaded' % CONFIG['parameters']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data loaders\n",
    "trainloader, valloader = get_train_valid_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init net\n",
    "net = ConvNet()\n",
    "\n",
    "# plot net parameters\n",
    "summary(net, input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize init weights\n",
    "w = net.conv1.weight.data.numpy()\n",
    "plot_weights(w,scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory\n",
    "name_run = CONFIG['parameters']['model'] + '_' + gethostname() + '_' + '%d' % time.time()\n",
    "filepath_run = os.path.join(CONFIG['parameters']['results'], name_run)\n",
    "Path(filepath_run).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# copy models.py and config file for documentation\n",
    "copy('models.py', os.path.join(filepath_run, 'models.py'))\n",
    "copy(filepath_config, os.path.join(filepath_run, 'config.json'))\n",
    "\n",
    "# set optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=CONFIG['parameters']['lr'], weight_decay=CONFIG['parameters']['l2_reg'])\n",
    "\n",
    "# https://discuss.pytorch.org/t/how-to-do-exponential-learning-rate-decay-in-pytorch/63146/3\n",
    "# https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, \n",
    "    factor=CONFIG['parameters']['lr_decay_rate'], \n",
    "    patience=CONFIG['parameters']['lr_decay_patience'], \n",
    "    threshold=CONFIG['parameters']['lr_decay_threshold'], \n",
    "    verbose = True)\n",
    "\n",
    "\n",
    "epochs = CONFIG['parameters']['epochs']\n",
    "running_loss = 0\n",
    "print_every = 200\n",
    "training_steps = 0\n",
    "\n",
    "# init tensorboard writer\n",
    "writer = SummaryWriter(log_dir=filepath_run)\n",
    "\n",
    "val_acc_best = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    print('\\n--- EPOCH %03d / %03d STARTED ---' % (e+1, epochs))\n",
    " \n",
    "    # set net to train mode\n",
    "    net.train()\n",
    "    \n",
    "    # log current learning rate\n",
    "    writer.add_scalar('optimizer/learning_rate', optimizer.param_groups[0]['lr'], e+1)\n",
    "    \n",
    "    # train over all batches\n",
    "    start = time.time()\n",
    "    for idx_batch, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        training_steps += 1\n",
    "        \n",
    "        if idx_batch % print_every == 0:\n",
    "            print('---> train on batch %03d' % idx_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('train/loss_training_step', loss.item(), training_steps)\n",
    "        \n",
    "    # visualize gradients   \n",
    "    plot_gradient_flow(net.named_parameters())\n",
    "\n",
    "    # set net to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # evaluate on training data \n",
    "    train_loss_accum = 0.\n",
    "    count_total = 0\n",
    "    count_correct = 0\n",
    "    for idx_batch, (images, labels) in enumerate(trainloader):\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss_accum += loss.item()\n",
    "        \n",
    "        prediction = torch.argmax(F.softmax(output), axis=1)\n",
    "        count_total += prediction.size(0)\n",
    "        count_correct += (prediction == labels).sum().item()\n",
    "        \n",
    "    train_loss_avg = train_loss_accum / (idx_batch+1)\n",
    "    train_acc = count_correct / count_total\n",
    "    writer.add_scalar('train/loss_epoch', train_loss_avg, e+1)\n",
    "    writer.add_scalar('train/accuracy', train_acc, e+1)\n",
    "    \n",
    "    # evaluate on validation data \n",
    "    val_loss_accum = 0.\n",
    "    count_total = 0\n",
    "    count_correct = 0\n",
    "    for idx_batch, (images, labels) in enumerate(valloader):\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        val_loss_accum += loss.item()\n",
    "        \n",
    "        prediction = torch.argmax(F.softmax(output), axis=1)\n",
    "        count_total += prediction.size(0)\n",
    "        count_correct += (prediction == labels).sum().item()\n",
    "\n",
    "    val_loss_avg = val_loss_accum / (idx_batch+1)\n",
    "    val_acc = count_correct / count_total\n",
    "    writer.add_scalar('val/loss_epoch', val_loss_avg, e+1)\n",
    "    writer.add_scalar('val/accuracy', val_acc, e+1)\n",
    "    \n",
    "    writer.add_scalar('val/train_val_loss_ratio', val_loss_avg-train_loss_avg, e+1)\n",
    "    \n",
    "    print('\\nMetrics: ')\n",
    "    print('---> train loss / accuracy: %.03f / %.03f' % (train_loss_avg, train_acc))\n",
    "    print('---> val loss / accuracy:   %.03f / %.03f' % (val_loss_avg, val_acc))\n",
    "    \n",
    "    scheduler.step(val_loss_avg)\n",
    "    \n",
    "    print('filters conv1')\n",
    "    w1 = net.conv1.weight.data.numpy()\n",
    "    plot_weights(w1, scaling=True)\n",
    "    plt.show()\n",
    "    \n",
    "    if val_acc > val_acc_best:\n",
    "        val_acc_best = val_acc\n",
    "        print('---> save new best checkpoint')\n",
    "        filepath_ckpt = os.path.join(filepath_run, '%s_epoch-%03d_train-%.03f_val-%.03f.ckpt' % (CONFIG['parameters']['model'],e+1,train_acc,val_acc))\n",
    "        torch.save(net.state_dict(), filepath_ckpt)\n",
    "\n",
    "# close tensorboard logging\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save best trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
