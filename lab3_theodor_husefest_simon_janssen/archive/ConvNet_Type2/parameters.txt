learning_rate = 1e-3
decayRate = 0.1
l2_reg = 0.01
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=l2_reg)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer=optimizer, 
    factor=decayRate, 
    patience=3, 
    threshold=1e-2, 
    verbose=True)


epochs = 50
running_loss = 0
print_every = 200
training_steps = 0
