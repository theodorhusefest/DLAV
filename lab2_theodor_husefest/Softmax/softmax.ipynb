{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "We will implement a softmax classifier that is trained on the CIFAR10 dataset. The output is a model that is able to classify the input image into the 10 different classes of the CIFAR10 dataset.\n",
    "\n",
    "Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.\n",
    "\n",
    "You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- use a validation set to **tune the learning rate** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '../data/cifar-10-batches-py'\n",
    "    X_train, y_train = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **softmax.py**. This is where the method **softmax_loss_vectorized** will be implemented. This method just returns the loss and gradient after applying the softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax import softmax_loss_vectorized\n",
    "\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_val, y_val)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the file linear_classifier.py, implement SGD in the function LinearClassifier.train() and call this function to do the training.\n",
    "## Write the LinearSVM.predict() function and call this function to evaluate the performance on both the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (learning rate). \n",
    "# You should experiment with different ranges for the learning\n",
    "# rates; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from linear_classifier import Softmax\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = Softmax()\n",
    "learning_rates = [5e-8, 1e-7, 5e-7]\n",
    "num_iters = 100\n",
    "losses = []\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning.                                  #\n",
    "# Save the best trained softmax classifer in best_softmax.                     #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def calculate_accuracy(y_pred, y_val):\n",
    "    return np.mean(y_pred == y_val)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    \n",
    "    # instantiate new softmax object\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    tic = time.time()\n",
    "    print('--- training cycle for learning rate %.03e ---' % lr)\n",
    "    \n",
    "    train_loss_history, val_loss_history = softmax.train(X_train, y_train, learning_rate = lr, num_iters=num_iters, batch_size= 200, verbose = True, reg = 1000, X_val=X_val, y_val=y_val)\n",
    "    loss_history = [train_loss_history, val_loss_history]\n",
    "    losses.append(loss_history)\n",
    "    \n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    \n",
    "    val_acc = calculate_accuracy(y_val_pred, y_val)\n",
    "    train_acc = calculate_accuracy(y_train_pred, y_train)\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        print('Updating best softmax')\n",
    "        best_val = val_acc\n",
    "        best_softmax.W = softmax.W.copy()\n",
    "        \n",
    "    \n",
    "    results[lr] = (train_acc, val_acc)\n",
    "    \n",
    "    toc = time.time()\n",
    "    print('\\nTraining executed in {}s'.format(toc-tic))\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[lr]\n",
    "    print('lr %.03e train accuracy: %f val accuracy: %.03f' % (\n",
    "                lr, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %.03f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = range(num_iters)\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    plt.title('Train vs. validation loss (learning rate: %.03e)' % lr)\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('training loss', color=color)\n",
    "    ax1.plot(iters, losses[i][0], color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('validation loss', color=color)\n",
    "    ax2.plot(iters, losses[i][1], color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on val set\n",
    "# Evaluate the best softmax on val set\n",
    "y_val_pred = best_softmax.predict(X_val)\n",
    "val_accuracy = np.mean(y_val == y_val_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (val_accuracy, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the best_softmax weights using pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('softmax_weights_acc_%.03f.pkl' % val_accuracy, 'wb') as f:\n",
    "    pickle.dump(softmax.W, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best_softmax weights using pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('softmax_weights_acc_%.03f.pkl' % val_accuracy, 'rb') as f:\n",
    "    W = pickle.load(f)\n",
    "new_softmax = Softmax()\n",
    "new_softmax.W = W.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w =  new_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
